{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a PDF of the Data Quality Report for Data Desginer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéõÔ∏è Import things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from navigator_helpers.llms.llm_suite import GretelLLMSuite\n",
    "from navigator_helpers.tasks.evaluation.evaluation import (\n",
    "    BaseEvaluationTaskSuite,\n",
    "    VisualizationTaskSuite\n",
    ")\n",
    "\n",
    "# set environment variable 'GRETEL_PROD_API_KEY' from https://console.gretel.ai/users/me/key\n",
    "gretel_prod_api_key = input(\"Enter your Gretel API key from https://console.gretel.ai/users/me/key: \")\n",
    "os.environ['GRETEL_PROD_API_KEY'] = gretel_prod_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¢ Choose Dataset for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of samples to load from the dataset for testing\n",
    "# Set to None to use the full dataset\n",
    "NUM_SAMPLES = 1000\n",
    "\n",
    "datasets_dict = {\n",
    "    \"synthetic_text_to_sql\": {\n",
    "        \"dataset_kwargs\": {\n",
    "            \"path\": \"gretelai/synthetic_text_to_sql\",\n",
    "            \"split\": \"train\"\n",
    "        },\n",
    "        \"code_lang\": \"sql\",\n",
    "        \"eval_kwargs\":{\n",
    "            \"instruction_col_name\": \"sql_prompt\",\n",
    "            \"code_col_name\": \"sql\",\n",
    "            \"context_col_name\": \"sql_context\"\n",
    "        }\n",
    "    },\n",
    "    \"gsm8k\": {\n",
    "        \"dataset_kwargs\": {\n",
    "            \"path\": \"openai/gsm8k\",\n",
    "            \"name\": \"main\",\n",
    "            \"split\": \"train\"\n",
    "        },\n",
    "        \"eval_kwargs\": {\n",
    "            \"instruction_col_name\": \"question\",\n",
    "            \"code_col_name\": \"answer\",\n",
    "        }\n",
    "    },\n",
    "    \"synthetic_gsm8k\": {\n",
    "        \"dataset_kwargs\": {\n",
    "            \"path\": \"gretelai/synthetic-gsm8k-reflection-405b\",\n",
    "            \"split\": \"train\"\n",
    "        },\n",
    "        \"eval_kwargs\": {\n",
    "            \"instruction_col_name\": \"question\",\n",
    "            \"code_col_name\": \"answer\",\n",
    "        }\n",
    "    },\n",
    "    \"xlcost_text_to_code\": {\n",
    "        \"dataset_kwargs\": {\n",
    "            \"path\": \"codeparrot/xlcost-text-to-code\",\n",
    "            \"split\": \"train\"\n",
    "        },\n",
    "        \"code_lang\": \"python\",\n",
    "        \"eval_kwargs\": {\n",
    "            \"instruction_col_name\": \"text\",\n",
    "            \"code_col_name\": \"code\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# Prompt user to select a dataset\n",
    "print(\"Available datasets:\")\n",
    "for key in datasets_dict.keys():\n",
    "    print(f\" - {key}\")\n",
    "\n",
    "selected_dataset = input(\"\\nEnter the name of the dataset to load: \").strip()\n",
    "\n",
    "# Load the selected dataset\n",
    "if selected_dataset in datasets_dict:\n",
    "    dataset_dict = datasets_dict[selected_dataset]\n",
    "    eval_kwargs = dataset_dict[\"eval_kwargs\"]\n",
    "    code_lang = dataset_dict[\"code_lang\"] if \"code_lang\" in dataset_dict.keys() else None\n",
    "    dataset = load_dataset(**dataset_dict[\"dataset_kwargs\"])\n",
    "\n",
    "    # Optionally, select a subset if NUM_SAMPLES is specified\n",
    "    if NUM_SAMPLES is not None and NUM_SAMPLES < len(dataset):\n",
    "        dataset = dataset.select(range(NUM_SAMPLES))\n",
    "    \n",
    "    dataset_df = dataset.to_pandas()\n",
    "    \n",
    "    print(f\"Loaded dataset '{selected_dataset}' successfully!\")\n",
    "else:\n",
    "    print(\"Error: Dataset not found. Please enter a valid dataset name.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tmp\n",
    "# import pandas as pd\n",
    "# dataset_df = pd.read_json('/mnt/foundation-shared/nina_xu_gretel_ai/datasets/text_to_python_v1.json')\n",
    "# dataset_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tmp\n",
    "# result = {'seed_results': {'row_uniqueness': {'percent_unique': 96.0,\n",
    "#    'percent_semantically_unique': 104.0,\n",
    "#    'non_unique_ids': [37, 40],\n",
    "#    'non_semantically_unique_ids': []},\n",
    "#   'feature_cardinality': {'domain': 0.2,\n",
    "#    'topic': 0.82,\n",
    "#    'complexity': 0.08,\n",
    "#    'dependency_list': 0.12},\n",
    "#   'feature_distribution': {'distribution': {'domain': {'Telecommunications': 8,\n",
    "#      'Educational Technology': 6,\n",
    "#      'Financial Services': 6,\n",
    "#      'Aerospace Software': 6,\n",
    "#      'E-commerce': 5,\n",
    "#      'Healthcare Technology': 5,\n",
    "#      'Video Game Development': 4,\n",
    "#      'Cybersecurity': 4,\n",
    "#      'Artificial Intelligence': 4,\n",
    "#      'Automotive Software': 2},\n",
    "#     'topic': {'avg_length': 20.4,\n",
    "#      'std_length': 4.8529393575608655,\n",
    "#      'avg_word_count': 2.32,\n",
    "#      'word_count_histogram': ([0, 0, 1, 0, 0, 34, 0, 13, 0, 2],\n",
    "#       [0.0,\n",
    "#        0.4,\n",
    "#        0.8,\n",
    "#        1.2000000000000002,\n",
    "#        1.6,\n",
    "#        2.0,\n",
    "#        2.4000000000000004,\n",
    "#        2.8000000000000003,\n",
    "#        3.2,\n",
    "#        3.6,\n",
    "#        4.0])},\n",
    "#     'complexity': {'Beginner: Basic syntax, data types, and control structures': 13,\n",
    "#      'Intermediate: Functions, modules, and file handling': 13,\n",
    "#      'Advanced: Object-oriented programming and exception handling': 13,\n",
    "#      'Expert: Concurrency, parallel processing, and metaprogramming': 11},\n",
    "#     'dependency_list': {\"['matplotlib', 'numpy', 'pandas', 'scikit-learn', 'seaborn']\": 26,\n",
    "#      \"['matplotlib', 'numpy', 'pandas', 'scikit-learn', 'tensorflow']\": 15,\n",
    "#      \"['matplotlib', 'numpy', 'pandas', 'scikit-learn', 'scipy']\": 5,\n",
    "#      \"['matplotlib', 'numpy', 'pandas', 'requests', 'scikit-learn']\": 2,\n",
    "#      \"['matplotlib', 'numpy', 'pandas', 'seaborn', 'sklearn']\": 1,\n",
    "#      \"['beautifulsoup4', 'numpy', 'pandas', 'requests', 'scikit-learn']\": 1}},\n",
    "#    'score': {'domain': {'gini_simpson_index': 0.8904},\n",
    "#     'topic': {'text_diversity_index': 0.791793559622543},\n",
    "#     'complexity': {'gini_simpson_index': 0.7488},\n",
    "#     'dependency_list': {'gini_simpson_index': 0.6272}},\n",
    "#    'column_data_types': {'domain': 'Categorical',\n",
    "#     'topic': 'Text',\n",
    "#     'complexity': 'Categorical',\n",
    "#     'dependency_list': 'Categorical'}},\n",
    "#   'num_words_per_record': {'average_words_per_record': 3.8949999999999996,\n",
    "#    'word_counts_per_column': {'domain': 1.74,\n",
    "#     'topic': 2.32,\n",
    "#     'complexity': 6.52,\n",
    "#     'dependency_list': 5.0},\n",
    "#    'average_tokens_per_record': 36.18,\n",
    "#    'total_tokens': 1809}},\n",
    "#  'results': {'row_uniqueness': {'percent_unique': 100.0,\n",
    "#    'percent_semantically_unique': 98.0,\n",
    "#    'non_unique_ids': [],\n",
    "#    'non_semantically_unique_ids': [(23, 37)]},\n",
    "#   'feature_cardinality': {'code': 1.0, 'prompt': 1.0},\n",
    "#   'feature_distribution': {'distribution': {'code': {'avg_length': 1112.28,\n",
    "#      'std_length': 689.2465285678682,\n",
    "#      'avg_word_count': 101.04,\n",
    "#      'word_count_histogram': ([7, 7, 7, 10, 6, 6, 3, 1, 1, 2],\n",
    "#       [0.0,\n",
    "#        27.7,\n",
    "#        55.4,\n",
    "#        83.1,\n",
    "#        110.8,\n",
    "#        138.5,\n",
    "#        166.2,\n",
    "#        193.9,\n",
    "#        221.6,\n",
    "#        249.29999999999998,\n",
    "#        277.0])},\n",
    "#     'prompt': {'avg_length': 1127.88,\n",
    "#      'std_length': 305.63067214385086,\n",
    "#      'avg_word_count': 165.1,\n",
    "#      'word_count_histogram': ([0, 0, 1, 26, 15, 3, 3, 0, 0, 2],\n",
    "#       [0.0,\n",
    "#        38.3,\n",
    "#        76.6,\n",
    "#        114.89999999999999,\n",
    "#        153.2,\n",
    "#        191.5,\n",
    "#        229.79999999999998,\n",
    "#        268.09999999999997,\n",
    "#        306.4,\n",
    "#        344.7,\n",
    "#        383.0])}},\n",
    "#    'score': {'code': {'text_diversity_index': 0.7266752740917914},\n",
    "#     'prompt': {'text_diversity_index': 0.5341946433368763}},\n",
    "#    'column_data_types': {'code': 'Text', 'prompt': 'Text'}},\n",
    "#   'num_words_per_record': {'average_words_per_record': 133.07,\n",
    "#    'word_counts_per_column': {'code': 101.04, 'prompt': 165.1},\n",
    "#    'average_tokens_per_record': 476.66,\n",
    "#    'total_tokens': 23833},\n",
    "#   'llm_as_a_critic_mean_scores': {'relevance_score': 3.92,\n",
    "#    'correctness_score': 3.9,\n",
    "#    'readability_score': 2.9,\n",
    "#    'efficiency_score': 3.02,\n",
    "#    'pythonic_score': 3.08},\n",
    "#   'valid_records_score': {'count': 45, 'percent': 0.9}},\n",
    "#  'dataset_overview_statistics': {'Number of Rows': 50,\n",
    "#   'Number of Columns': 2,\n",
    "#   'Categorical Columns': 0,\n",
    "#   'Text Columns': 2,\n",
    "#   'Numerical Columns': 0,\n",
    "#   'Other Columns': 0,\n",
    "#   'Data Completeness': 100.0}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_suite = GretelLLMSuite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to store evaluation results\n",
    "results = {}\n",
    "\n",
    "# Uncomment the following lines to run individual evaluation tasks\n",
    "results.update({\"row_uniqueness\": BaseEvaluationTaskSuite(llm_suite, dataset_df).row_uniqueness()})\n",
    "results.update({\"feature_cardinality\": BaseEvaluationTaskSuite(llm_suite, dataset_df).feature_cardinality()})\n",
    "results.update({\"feature_distribution\": BaseEvaluationTaskSuite(llm_suite, dataset_df).feature_distribution()})\n",
    "results.update({\"num_words_per_record\": BaseEvaluationTaskSuite(llm_suite, dataset_df).num_words_per_record()})\n",
    "\n",
    "# Uncomment this line to run everything, including LLM-as-a-judge\n",
    "# results = BaseEvaluationTaskSuite(llm_suite, dataset_df, code_lang, eval_kwargs).evaluate_all()\n",
    "\n",
    "pprint(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from typing import List, Optional, Tuple, Union, Dict, Any\n",
    "import math\n",
    "import io\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.io import to_image\n",
    "from PIL import Image as PILImage\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, PageBreak, Paragraph, Spacer, Image, Table, TableStyle\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.lib.enums import TA_CENTER\n",
    "\n",
    "# Constants\n",
    "FIG_WIDTH = 7  # inches\n",
    "FIG_HEIGHT = 3.6  # inches\n",
    "SCORE_VALUES = [\n",
    "    {\"label\": \"Very poor\", \"color\": \"rgb(229, 60, 26)\"},\n",
    "    {\"label\": \"Poor\", \"color\": \"rgb(229, 128, 26)\"},\n",
    "    {\"label\": \"Average\", \"color\": \"rgb(229, 161, 26)\"},\n",
    "    {\"label\": \"Good\", \"color\": \"rgb(183, 210, 45)\"},\n",
    "    {\"label\": \"Excellent\", \"color\": \"rgb(72, 210, 45)\"},\n",
    "]\n",
    "PRIMARY_PALETTE = ['#2E1065', '#D3A66E', '#110420', '#4F00A9', '#F9EFDE', '#1D0B32', '#8D32FA', '#C399FF', '#EFE5FF', '#EFD7AD', '#F4E3C6', '#FBF7ED', '#A59DAD', '#D2CED6', '#E8E7EB']\n",
    "SECONDARY_PALETTE = ['#052095', '#FF6BA9', '#3056F2', '#FFA8CC', '#8BB9FF', '#FFEDF5', '#E5F0FF', '#1E9C98', '#92F6F4', '#C5FEFF', '#E8FEFF', '#FF9248', '#FFB38A', '#FFD7B5', '#FFECDC', '#FF6700', '#FFCA1A', '#FFE16D', '#FFF099', '#FFFDE3', '#ECA10A']\n",
    "\n",
    "# Set up custom color palette for seaborn\n",
    "sns.set_theme(style=\"white\")\n",
    "sns.set_palette(sns.color_palette(SECONDARY_PALETTE))\n",
    "\n",
    "def create_chart(data: pd.Series, title: str, xlabel: str, ylabel: str) -> Image:\n",
    "    fig, ax = plt.subplots(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "    bars = ax.bar(range(len(data)), data.values, color='#4F00A9')\n",
    "    ax.set_facecolor('white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    ax.set_title(title, fontsize=10, color='#1D0B32')\n",
    "    ax.set_xlabel(xlabel, fontsize=10, color='#1D0B32')\n",
    "    ax.set_ylabel(ylabel, fontsize=10, color='#1D0B32')\n",
    "    ax.set_xticks(range(len(data)))\n",
    "    \n",
    "    truncated_labels = [str(label)[:17] + '...' if len(str(label)) > 20 else str(label) for label in data.index]\n",
    "    ax.set_xticklabels(truncated_labels, rotation=45, ha='right', fontsize=8, color='#1D0B32')\n",
    "    \n",
    "    ax.tick_params(axis='both', colors='#1D0B32')\n",
    "    ax.tick_params(axis='y', labelsize=6)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}' if isinstance(height, float) else f'{height}',\n",
    "                ha='center', va='bottom', fontsize=8, color='#1D0B32')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    img_buffer = io.BytesIO()\n",
    "    plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')\n",
    "    img_buffer.seek(0)\n",
    "    return Image(img_buffer, width=7*inch, height=4*inch)\n",
    "\n",
    "def create_pareto_chart(data: pd.DataFrame, title: str) -> Image:\n",
    "    fig, ax1 = plt.subplots(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    bars = ax1.bar(range(len(data)), data['count'], color='#4F00A9')\n",
    "    ax1.set_facecolor('white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    ax1.set_xlabel('Categories', fontsize=10, color='#1D0B32')\n",
    "    ax1.set_ylabel('Count', fontsize=10, color='#1D0B32')\n",
    "    ax1.set_title(title, fontsize=10, color='#1D0B32')\n",
    "    \n",
    "    cumulative_percentage = 100 * data['count'].cumsum() / data['count'].sum()\n",
    "    ax2.plot(range(len(data)), cumulative_percentage, color='#FF6700', marker='D', ms=4)\n",
    "    ax2.set_ylabel('Cumulative Percentage', fontsize=10, color='#1D0B32')\n",
    "    ax2.set_ylim([0, 110])\n",
    "    \n",
    "    ax1.tick_params(axis='both', colors='#1D0B32')\n",
    "    ax2.tick_params(axis='both', colors='#1D0B32')\n",
    "    ax1.tick_params(axis='y', labelsize=6)\n",
    "    ax2.tick_params(axis='y', labelsize=6)\n",
    "    \n",
    "    ax1.set_xticks(range(len(data)))\n",
    "    truncated_labels = [str(label)[:17] + '...' if len(str(label)) > 20 else str(label) for label in data.index]\n",
    "    ax1.set_xticklabels(truncated_labels, rotation=45, ha='right', fontsize=6, color='#1D0B32')\n",
    "    \n",
    "    for i, v in enumerate(data['count']):\n",
    "        ax1.text(i, v, f'{v:.2f}' if isinstance(v, float) else f'{v}', ha='center', va='bottom', fontsize=8, color='#1D0B32')\n",
    "    \n",
    "    ax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0f}%'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    img_buffer = io.BytesIO()\n",
    "    plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')\n",
    "    img_buffer.seek(0)\n",
    "    return Image(img_buffer, width=FIG_WIDTH*inch, height=FIG_HEIGHT*inch)\n",
    "\n",
    "def create_text_diversity_chart(text_diversity_df: pd.DataFrame) -> Image:\n",
    "    plt.figure(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "    ax = sns.barplot(x=text_diversity_df.index, y='diversity_index', data=text_diversity_df, color='#4F00A9')\n",
    "    ax.set_facecolor('white')\n",
    "    plt.gcf().patch.set_facecolor('white')\n",
    "    \n",
    "    plt.title(\"Text Diversity Indices\", fontsize=10, color='#1D0B32')\n",
    "    plt.ylabel(\"Diversity Index\", fontsize=10, color='#1D0B32')\n",
    "    plt.xlabel(\"\", fontsize=10, color='#1D0B32')\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=8, color='#1D0B32')\n",
    "    \n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    ax.tick_params(axis='both', colors='#1D0B32')\n",
    "    ax.tick_params(axis='y', labelsize=6)\n",
    "    \n",
    "    for i, v in enumerate(text_diversity_df['diversity_index']):\n",
    "        ax.text(i, v, f'{v:.2f}', ha='center', va='bottom', fontsize=8, color='#1D0B32')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    img_buffer = io.BytesIO()\n",
    "    plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')\n",
    "    img_buffer.seek(0)\n",
    "    return Image(img_buffer, width=FIG_WIDTH*inch, height=FIG_HEIGHT*inch)\n",
    "\n",
    "def create_histogram(counts: List[int], bins: List[float], col_name: str, data_type: str = \"Text\") -> Image:\n",
    "    assert data_type in [\"Text\", \"Numeric\"], f\"Invalid data type: {data_type}\"\n",
    "    if data_type == \"Text\":\n",
    "        x_label = \"Word Count\"\n",
    "    else:\n",
    "        x_label = \"Value\"\n",
    "    \n",
    "    plt.figure(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "    plt.hist(bins[:-1], bins, weights=counts, color='#4F00A9')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(x_label, fontsize=10, color='#1D0B32')\n",
    "    plt.ylabel(\"Count\", fontsize=10, color='#1D0B32')\n",
    "    plt.title(f'{col_name.replace('_', ' ').title()} {x_label} Distribution (Histogram)', fontsize=10, color='#1D0B32')\n",
    "    plt.xticks(fontsize=6, color='#1D0B32')\n",
    "    plt.yticks(fontsize=6, color='#1D0B32')\n",
    "\n",
    "    # Add counts above bars\n",
    "    for i in range(len(counts)):\n",
    "        plt.text((bins[i]+bins[i+1])/2, counts[i], str(counts[i]), ha='center', va='bottom', fontsize=8, color='#1D0B32')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    img_buffer = io.BytesIO()\n",
    "    plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')\n",
    "    img_buffer.seek(0)\n",
    "    return Image(img_buffer, width=FIG_WIDTH*inch, height=FIG_HEIGHT*inch)\n",
    "\n",
    "def create_schema_table(dataset_df: pd.DataFrame, data: Dict[str, Any]) -> Tuple[Table, Dict[str, float]]:\n",
    "    schema_data = [['Column Name', 'Data Type', 'Total Count', '% Null', 'Average Length', 'Avg Tokens']]\n",
    "    for col in dataset_df.columns:\n",
    "        dtype = data['feature_distribution']['column_data_types'][col] if 'feature_distribution' in data and 'column_data_types' in data['feature_distribution'] else 'N/A'\n",
    "        total_count = len(dataset_df)\n",
    "        null_count = dataset_df[col].isnull().sum()\n",
    "        pcnt_null = (null_count / total_count) * 100\n",
    "        avg_length = 'N/A'\n",
    "        avg_tokens = 'N/A'\n",
    "        if 'feature_distribution' in data and 'distribution' in data['feature_distribution']:\n",
    "            if col in data['feature_distribution']['distribution']:\n",
    "                col_data = data['feature_distribution']['distribution'][col]\n",
    "                if isinstance(col_data, dict):\n",
    "                    if 'avg_length' in col_data:\n",
    "                        avg_length = f\"{col_data['avg_length']:.2f}\"\n",
    "                    if 'avg_tokens' in col_data:\n",
    "                        avg_tokens = f\"{col_data['avg_tokens']:.2f}\"\n",
    "        schema_data.append([col, dtype, total_count, f\"{pcnt_null:.2f}%\", avg_length, avg_tokens])\n",
    "    \n",
    "    table = Table(schema_data)\n",
    "    style = TableStyle([\n",
    "        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#4F00A9')),\n",
    "        ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),\n",
    "        ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
    "        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "        ('FONTSIZE', (0, 0), (-1, 0), 8),\n",
    "        ('BOTTOMPADDING', (0, 0), (-1, 0), 6),\n",
    "        ('BACKGROUND', (0, 1), (-1, -1), colors.HexColor('#EFE5FF')),\n",
    "        ('TEXTCOLOR', (0, 1), (-1, -1), colors.HexColor('#110420')),\n",
    "        ('ALIGN', (0, 1), (-1, -1), 'LEFT'),\n",
    "        ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),\n",
    "        ('FONTSIZE', (0, 1), (-1, -1), 7),\n",
    "        ('TOPPADDING', (0, 1), (-1, -1), 3),\n",
    "        ('BOTTOMPADDING', (0, 1), (-1, -1), 3),\n",
    "        ('GRID', (0, 0), (-1, -1), 1, colors.HexColor('#4F00A9'))\n",
    "    ])\n",
    "    table.setStyle(style)\n",
    "    return table\n",
    "\n",
    "def create_overview_table(overview_data: List[List[str]]) -> Table:\n",
    "    table = Table(overview_data, colWidths=[1.5*inch, 1.5*inch])\n",
    "    style = TableStyle([\n",
    "        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#4F00A9')),\n",
    "        ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),\n",
    "        ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
    "        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "        ('FONTSIZE', (0, 0), (-1, 0), 8),\n",
    "        ('BOTTOMPADDING', (0, 0), (-1, 0), 3),  # Reduced padding\n",
    "        ('BACKGROUND', (0, 1), (-1, -1), colors.HexColor('#EFE5FF')),\n",
    "        ('TEXTCOLOR', (0, 1), (-1, -1), colors.HexColor('#110420')),\n",
    "        ('ALIGN', (0, 1), (-1, -1), 'LEFT'),\n",
    "        ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),\n",
    "        ('FONTSIZE', (0, 1), (-1, -1), 7),\n",
    "        ('TOPPADDING', (0, 1), (-1, -1), 3),  # Minimal top padding\n",
    "        ('BOTTOMPADDING', (0, 1), (-1, -1), 3),  # Minimal bottom padding\n",
    "        ('GRID', (0, 0), (-1, -1), 0.5, colors.HexColor('#4F00A9'))  # Thinner grid lines\n",
    "    ])\n",
    "    table.setStyle(style)\n",
    "    return table\n",
    "\n",
    "def create_single_record_preview(dataset_df: pd.DataFrame) -> str:\n",
    "    row = dataset_df.iloc[0]\n",
    "    preview_text = \"\"\n",
    "    for column, value in row.items():\n",
    "        truncated_value = str(value)[:100] + ('...' if len(str(value)) > 100 else '')\n",
    "        preview_text += f\"<b>{column}:</b>\\t{truncated_value}\"\n",
    "        preview_text += \"<br/>\"\n",
    "    return preview_text\n",
    "\n",
    "def _generate_pointer_path(score: int) -> str:\n",
    "    theta = score * (282 - 34) / 100 - 34\n",
    "    rads = math.radians(theta)\n",
    "    radius = 0.45\n",
    "    size = 0.025\n",
    "    x1 = -1 * radius * math.cos(rads) + 0.5\n",
    "    y1 = radius * math.sin(rads) + 0.5\n",
    "    return f\"\"\"\n",
    "    M {x1} {y1}\n",
    "    L {-1 * size * math.cos(math.radians(theta - 90)) + 0.5}\n",
    "        {size * math.sin(math.radians(theta - 90)) + 0.5}\n",
    "    L {-1 * size * math.cos(math.radians(theta + 90)) + 0.5}\n",
    "        {size * math.sin(math.radians(theta + 90)) + 0.5}\n",
    "    Z\"\"\"\n",
    "\n",
    "def gauge_and_needle_chart(score: Optional[int], display_score: bool = True, marker_colors: Optional[List[str]] = None) -> go.Figure:\n",
    "    if score is None:\n",
    "        fig = go.Figure(\n",
    "            layout=go.Layout(\n",
    "                annotations=[\n",
    "                    go.layout.Annotation(\n",
    "                        text=\"N/A\",\n",
    "                        font=dict(color=\"rgba(174, 95, 5, 1)\", size=18),\n",
    "                        showarrow=False,\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\",\n",
    "                        x=0.5,\n",
    "                        y=0.5,\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        marker_colors = [\"rgb(220, 220, 220)\", \"rgba(255, 255, 255, 0)\"]\n",
    "        pie_values = [70, 30]\n",
    "    else:\n",
    "        if not marker_colors:\n",
    "            marker_colors = [s[\"color\"] for s in SCORE_VALUES]\n",
    "        if marker_colors[-1] != \"rgba(255, 255, 255, 0)\":\n",
    "            marker_colors.append(\"rgba(255, 255, 255, 0)\")\n",
    "        pie_values = [70 // (len(marker_colors) - 1)] * (len(marker_colors) - 1)\n",
    "        pie_values.append(30)\n",
    "        fig = go.Figure()\n",
    "\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        showlegend=False,\n",
    "        xaxis=dict(visible=False),\n",
    "        yaxis=dict(visible=False),\n",
    "        height=180,\n",
    "        width=180,\n",
    "        margin=dict(l=0, r=0, t=0, b=0),\n",
    "        paper_bgcolor=\"rgba(0,0,0,0)\",\n",
    "        hovermode=False,\n",
    "        modebar=None,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Pie(\n",
    "            name=\"gauge\",\n",
    "            values=pie_values,\n",
    "            marker=dict(\n",
    "                colors=marker_colors,\n",
    "                line=dict(width=4, color=\"#fafafa\"),\n",
    "            ),\n",
    "            hole=0.75,\n",
    "            direction=\"clockwise\",\n",
    "            sort=False,\n",
    "            rotation=234,\n",
    "            showlegend=False,\n",
    "            hoverinfo=\"none\",\n",
    "            textinfo=\"none\",\n",
    "            textposition=\"outside\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if score is not None:\n",
    "        if display_score:\n",
    "            fig.add_trace(\n",
    "                go.Indicator(\n",
    "                    mode=\"number\", value=score, domain=dict(x=[0, 1], y=[0.28, 0.45])\n",
    "                )\n",
    "            )\n",
    "        fig.add_shape(\n",
    "            type=\"circle\", fillcolor=\"black\", x0=0.475, x1=0.525, y0=0.475, y1=0.525\n",
    "        )\n",
    "        fig.add_shape(\n",
    "            type=\"path\",\n",
    "            fillcolor=\"black\",\n",
    "            line=dict(width=0),\n",
    "            path=_generate_pointer_path(score),\n",
    "        )\n",
    "\n",
    "    return fig\n",
    "\n",
    "def create_gauge_chart(score: int) -> Image:\n",
    "    fig = gauge_and_needle_chart(score)\n",
    "    fig.update_layout(\n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        plot_bgcolor='rgba(0,0,0,0)',\n",
    "        margin=dict(t=20, b=20, l=20, r=20)\n",
    "    )\n",
    "    img_bytes = to_image(fig, format=\"png\", scale=2)\n",
    "    img = PILImage.open(io.BytesIO(img_bytes))\n",
    "    img_buffer = io.BytesIO()\n",
    "    img.save(img_buffer, format=\"PNG\")\n",
    "    img_buffer.seek(0)\n",
    "    return Image(img_buffer, width=1.75*inch, height=1.75*inch)\n",
    "\n",
    "def create_report_pdf(data: Dict[str, Any], dataset_df: pd.DataFrame, output_filename: str = 'enhanced_data_quality_report.pdf'):\n",
    "    doc = SimpleDocTemplate(output_filename, pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "    \n",
    "    chart_title_style = ParagraphStyle(\n",
    "        name='ChartTitle', \n",
    "        parent=styles['BodyText'], \n",
    "        alignment=TA_CENTER,\n",
    "        fontSize=8,\n",
    "        leading=10\n",
    "    )\n",
    "    styles.add(chart_title_style)\n",
    "\n",
    "    styles['Title'].fontSize = 24\n",
    "    styles['Title'].alignment = 1\n",
    "    styles['Title'].spaceAfter = 12\n",
    "    styles['Title'].textColor = colors.HexColor('#110420')\n",
    "\n",
    "    styles['Heading1'].fontSize = 18\n",
    "    styles['Heading1'].spaceAfter = 6\n",
    "    styles['Heading1'].textColor = colors.HexColor('#110420')\n",
    "\n",
    "    styles['Heading2'].fontSize = 14\n",
    "    styles['Heading2'].spaceBefore = 12\n",
    "    styles['Heading2'].spaceAfter = 6\n",
    "    styles['Heading2'].textColor = colors.HexColor('#110420')\n",
    "\n",
    "    styles['BodyText'].fontSize = 10\n",
    "    styles['BodyText'].spaceBefore = 6\n",
    "    styles['BodyText'].spaceAfter = 6\n",
    "    styles['BodyText'].textColor = colors.HexColor('#110420')\n",
    "\n",
    "    styles.add(ParagraphStyle(name='RowPreview',\n",
    "                              parent=styles['BodyText'],\n",
    "                              fontName='Courier',\n",
    "                              fontSize=8,\n",
    "                              leading=10,\n",
    "                              spaceAfter=12,\n",
    "                              firstLineIndent=0,\n",
    "                              leftIndent=20))\n",
    "    \n",
    "    story = []\n",
    "\n",
    "    data_completeness = (1 - dataset_df.isnull().sum().sum() / (dataset_df.shape[0] * dataset_df.shape[1])) * 100\n",
    "    \n",
    "    text_diversity_scores = []\n",
    "    gini_scores = []\n",
    "    if 'feature_distribution' in data and 'score' in data['feature_distribution']:\n",
    "        for value in data['feature_distribution']['score'].values():\n",
    "            if isinstance(value, dict):\n",
    "                if 'text_diversity_index' in value:\n",
    "                    text_diversity_scores.append(value['text_diversity_index'])\n",
    "                if 'gini_simpson_index' in value:\n",
    "                    gini_scores.append(value['gini_simpson_index'])\n",
    "    \n",
    "    avg_text_diversity = sum(text_diversity_scores) / len(text_diversity_scores) if text_diversity_scores else 0\n",
    "    avg_gini_index = sum(gini_scores) / len(gini_scores) if gini_scores else 0\n",
    "\n",
    "    story.append(Paragraph(\"Data Quality Report\", styles['Title']))\n",
    "    story.append(Spacer(1, 0.2*inch))\n",
    "\n",
    "    story.append(Paragraph(\"Key Metrics\", styles['Heading1']))\n",
    "    \n",
    "    unique_rows_chart = create_gauge_chart(int(data['row_uniqueness']['percent_unique']))\n",
    "    semantically_unique_rows_chart = create_gauge_chart(int(data['row_uniqueness']['percent_semantically_unique']))\n",
    "    text_diversity_chart = create_gauge_chart(int(avg_text_diversity * 100))\n",
    "    gini_simpson_chart = create_gauge_chart(int(avg_gini_index * 100))\n",
    "\n",
    "    unique_rows_title = Paragraph(\"Unique Rows\", styles['ChartTitle'])\n",
    "    semantically_unique_rows_title = Paragraph(\"Semantically Unique Rows\", styles['ChartTitle'])\n",
    "    text_diversity_title = Paragraph(\"Text Diversity\", styles['ChartTitle'])\n",
    "    gini_simpson_title = Paragraph(\"Gini-Simpson Diversity\", styles['ChartTitle'])\n",
    "\n",
    "    chart_table = Table([\n",
    "        [unique_rows_title, semantically_unique_rows_title, text_diversity_title, gini_simpson_title],\n",
    "        [unique_rows_chart, semantically_unique_rows_chart, text_diversity_chart, gini_simpson_chart]\n",
    "    ])\n",
    "    chart_table_style = TableStyle([\n",
    "        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
    "        ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),\n",
    "        ('BOTTOMPADDING', (0, 0), (-1, 0), 2), \n",
    "        ('TOPPADDING', (0, 1), (-1, -1), 3),\n",
    "    ])\n",
    "    chart_table.setStyle(chart_table_style)\n",
    "\n",
    "    story.append(chart_table)\n",
    "\n",
    "    # Dataset Overview\n",
    "    story.append(Paragraph(\"Dataset Overview\", styles['Heading1']))\n",
    "    story.append(Paragraph(\"This section provides key metrics on the structure, uniqueness, complexity, and quality of the data:\", styles['BodyText']))\n",
    "\n",
    "    data_types = list(data['feature_distribution']['column_data_types'].values())\n",
    "    # Split the overview data into two tables to save space\n",
    "    overview_data_1 = [\n",
    "        [\"Metric\", \"Value\"],\n",
    "        [\"Data Completeness\", f\"{data_completeness:.2f}%\"],\n",
    "        [\"Number of Rows\", f\"{len(dataset_df)}\"],\n",
    "        [\"Number of Columns\", f\"{len(dataset_df.columns)}\"],\n",
    "        [\"Categorical Columns\", f\"{data_types.count('Categorical')}\"],\n",
    "        [\"Text Columns\", f\"{data_types.count('Text')}\"],\n",
    "        [\"Numerical Columns\", f\"{data_types.count('Numeric')}\"],\n",
    "        [\"Other Columns\", f\"{data_types.count('Other')}\"],\n",
    "    ]\n",
    "\n",
    "    overview_data_2 = [\n",
    "        [\"Metric\", \"Value\"],\n",
    "        [\"Unique Rows\", f\"{data['row_uniqueness']['percent_unique']}%\"],\n",
    "        [\"Semantically Unique Rows\", f\"{data['row_uniqueness']['percent_semantically_unique']}%\"],\n",
    "        [\"Avg Words per Row\", f\"{data['num_words_per_record']['average_words_per_record']:.2f}\"],\n",
    "        [\"Avg Tokens per Row\", \"N/A\"],\n",
    "        [\"Total Tokens\", \"N/A\"],\n",
    "        [\"Avg Text Diversity\", f\"{avg_text_diversity:.4f}\"],\n",
    "        [\"Avg Gini-Simpson Index\", f\"{avg_gini_index:.4f}\"],\n",
    "    ]\n",
    "\n",
    "    overview_table_1 = create_overview_table(overview_data_1)\n",
    "    overview_table_2 = create_overview_table(overview_data_2)\n",
    "    overview_table_table = Table([[overview_table_1, overview_table_2]])\n",
    "    story.append(overview_table_table)\n",
    "    story.append(Spacer(1, 0.2*inch))\n",
    "\n",
    "    # Single Row Preview\n",
    "    # TODO: cut off the preview if it would spill into the next page?\n",
    "    story.append(Paragraph(\"Single Row Preview\", styles['Heading1']))\n",
    "    preview_text = create_single_record_preview(dataset_df)\n",
    "    story.append(Paragraph(preview_text, styles['RowPreview']))\n",
    "    story.append(PageBreak())\n",
    "\n",
    "    # Dataset Schema\n",
    "    story.append(Paragraph(\"Dataset Schema & Preview\", styles['Heading1']))\n",
    "    story.append(Paragraph(\"The schema table provides an overview of each column in the dataset.\", styles['BodyText']))\n",
    "    schema_table = create_schema_table(dataset_df, data)\n",
    "    story.append(schema_table)\n",
    "    story.append(Spacer(1, 0.2*inch))\n",
    "\n",
    "    # column Cardinality\n",
    "    if 'feature_cardinality' in data:\n",
    "        story.append(Paragraph(\"Column Cardinality\", styles['Heading1']))\n",
    "        feature_cardinality = pd.DataFrame.from_dict(data['feature_cardinality'], orient='index', columns=['cardinality'])\n",
    "        \n",
    "        img = create_chart(feature_cardinality['cardinality'], \"Column Cardinality\", \"Columns\", \"Cardinality\")\n",
    "        story.append(img)\n",
    "        story.append(PageBreak())\n",
    "        plot_count = 1\n",
    "    else:\n",
    "        plot_count = 0\n",
    "\n",
    "    # Distribution Visualizations\n",
    "    if 'feature_distribution' in data and 'distribution' in data['feature_distribution']:\n",
    "        for key, distribution in data['feature_distribution']['distribution'].items():\n",
    "            \n",
    "            dtype = data['feature_distribution']['column_data_types'][key] if 'feature_distribution' in data and 'column_data_types' in data['feature_distribution'] else None\n",
    "        \n",
    "            if distribution and isinstance(distribution, dict):\n",
    "                try:\n",
    "                    story.append(Paragraph(f\"{key.replace('_', ' ').title()} Distribution\", styles['Heading1']))\n",
    "                    if 'score' in data['feature_distribution'] and key in data['feature_distribution']['score']:\n",
    "                        for score_key, score_value in data['feature_distribution']['score'][key].items():\n",
    "                            story.append(Paragraph(f\"{score_key.replace('_', ' ').title()}: {score_value:.2f}\", styles['BodyText']))\n",
    "                    \n",
    "                    if dtype == 'Categorical':\n",
    "                        dist_df = pd.DataFrame.from_dict(distribution, orient='index', columns=['count'])\n",
    "                        dist_df['count'] = pd.to_numeric(dist_df['count'], errors='coerce')\n",
    "                        dist_df = dist_df.dropna().sort_values('count', ascending=False)\n",
    "                        \n",
    "                        if not dist_df.empty:\n",
    "                            # Handle large distributions\n",
    "                            if len(dist_df) > 75:\n",
    "                                other_count = dist_df.iloc[75:]['count'].sum()\n",
    "                                dist_df = dist_df.iloc[:75]\n",
    "                                dist_df.loc['Other'] = other_count\n",
    "                            img = create_pareto_chart(dist_df, f\"{key.replace('_', ' ').title()} Distribution (Pareto Chart)\")\n",
    "                        else:\n",
    "                            continue\n",
    "                    \n",
    "                    elif dtype == 'Text':\n",
    "                        counts, bins = distribution['word_count_histogram']\n",
    "                        img = create_histogram(counts, bins, key, \"Text\")\n",
    "                        \n",
    "                    elif dtype == 'Numeric':\n",
    "                        counts = distribution['histogram']\n",
    "                        bins = distribution['bin_edges']\n",
    "                        img = create_histogram(counts, bins, key, \"Numeric\")\n",
    "                    else:\n",
    "                        # Skip unsupported column types, e.g., 'Other', None\n",
    "                        continue\n",
    "\n",
    "                    story.append(img)\n",
    "                    plot_count += 1\n",
    "\n",
    "                    # Fit 2 plots per page\n",
    "                    if plot_count % 2 == 0:\n",
    "                        story.append(PageBreak())\n",
    "                    else:\n",
    "                        story.append(Spacer(1, 0.2*inch))\n",
    "                except Exception as e:\n",
    "                    story.append(Paragraph(f\"Error processing {key} distribution: {str(e)}\", styles['BodyText']))\n",
    "\n",
    "    # Word Count per Column\n",
    "    if 'word_counts_per_column' in data['num_words_per_record']:\n",
    "        story.append(Paragraph(\"Average Word Count per Column\", styles['Heading1']))\n",
    "        word_count = pd.DataFrame.from_dict(data['num_words_per_record']['word_counts_per_column'], orient='index', columns=['avg_words'])\n",
    "        word_count = word_count.sort_values('avg_words', ascending=False)\n",
    "        \n",
    "        img = create_chart(word_count['avg_words'], \"Average Word Count per Column\", \"Columns\", \"Average Word Count\")\n",
    "        story.append(img)\n",
    "        story.append(Spacer(1, 0.2*inch))\n",
    "\n",
    "    # Text Diversity Indices\n",
    "    if 'feature_distribution' in data and 'score' in data['feature_distribution']:\n",
    "        text_diversity = {}\n",
    "        for key, value in data['feature_distribution']['score'].items():\n",
    "            if isinstance(value, dict) and 'text_diversity_index' in value:\n",
    "                text_diversity[key] = value['text_diversity_index']\n",
    "        if text_diversity:\n",
    "            story.append(Paragraph(\"Text Diversity Indices\", styles['Heading1']))\n",
    "            text_diversity_df = pd.DataFrame.from_dict(text_diversity, orient='index', columns=['diversity_index'])\n",
    "            img = create_text_diversity_chart(text_diversity_df)\n",
    "            story.append(img)\n",
    "            story.append(Spacer(1, 0.2*inch))\n",
    "            \n",
    "\n",
    "    # Conclusion\n",
    "    story.append(Paragraph(\"Conclusion\", styles['Heading1']))\n",
    "\n",
    "    conclusion_text = \"This report provides a comprehensive view of the dataset's structure, content diversity, and the nature of the data it contains. Key takeaways include:<br/>\"\n",
    "\n",
    "    # Data Uniqueness\n",
    "    if 'row_uniqueness' in data:\n",
    "        unique = data['row_uniqueness'].get('percent_unique', 'N/A')\n",
    "        sem_unique = data['row_uniqueness'].get('percent_semantically_unique', 'N/A')\n",
    "        conclusion_text += f\"1. Data Uniqueness: With {unique}% unique rows and {sem_unique}% semantically unique rows, \"\n",
    "        if unique != 'N/A' and float(unique) > 90:\n",
    "            conclusion_text += \"the dataset shows a high degree of individuality in its rows. This suggests a rich and varied dataset.<br/><br/>\"\n",
    "        else:\n",
    "            conclusion_text += \"the dataset shows some level of repetition in its rows. This may indicate patterns or recurring themes in the data.<br/><br/>\"\n",
    "\n",
    "    # column Cardinality\n",
    "    if 'feature_cardinality' in data:\n",
    "        conclusion_text += \"2. Column Cardinality: The dataset contains columns with varying cardinalities. \"\n",
    "        conclusion_text += \"This diversity in column types allows for both granular analysis and higher-level pattern recognition.<br/><br/>\"\n",
    "\n",
    "    # Distribution Patterns\n",
    "    if 'feature_distribution' in data and 'distribution' in data['feature_distribution']:\n",
    "        conclusion_text += \"3. Distribution Patterns: The charts reveal the distribution patterns within each column, \"\n",
    "        conclusion_text += \"highlighting potential focus areas or biases in the data. Understanding these distributions \"\n",
    "        conclusion_text += \"is crucial for balanced analysis and identifying underrepresented categories.<br/><br/>\"\n",
    "\n",
    "    # Text Complexity\n",
    "    if 'num_words_per_record' in data:\n",
    "        avg_words = data['num_words_per_record'].get('average_words_per_record', 'N/A')\n",
    "        if avg_words != 'N/A':\n",
    "            conclusion_text += f\"4. Text Complexity: With an average of {avg_words:.2f} words per row, \"\n",
    "            if float(avg_words) > 50:\n",
    "                conclusion_text += \"the dataset shows a high level of complexity. \"\n",
    "            elif float(avg_words) > 20:\n",
    "                conclusion_text += \"the dataset shows a moderate level of complexity. \"\n",
    "            else:\n",
    "                conclusion_text += \"the dataset shows a low level of complexity. \"\n",
    "            conclusion_text += \"This gives an indication of the depth of information contained in each row.<br/><br/>\"\n",
    "\n",
    "    # Text Diversity\n",
    "    if 'feature_distribution' in data and 'score' in data['feature_distribution']:\n",
    "        conclusion_text += \"5. Text Diversity: The text diversity indices provide insight into the variety of content within text columns. \"\n",
    "        conclusion_text += \"Higher diversity can be beneficial for tasks requiring a broad range of examples, while lower diversity \"\n",
    "        conclusion_text += \"might indicate more standardized content.<br/><br/>\"\n",
    "\n",
    "    conclusion_text += \"\"\"\n",
    "    <b>Implications for Machine Learning:</b><br/><br/> \n",
    " \n",
    "    <b>Pre-training</b><br/> \n",
    "    - The dataset's uniqueness and diversity can provide a rich foundation for pre-training language models or other AI systems.<br/>\n",
    "    - High cardinality columns may help in learning broad representations, while low cardinality columns could aid in learning important categorical distinctions.<br/>\n",
    "    - If text diversity is high, it could be particularly valuable for building robust language models that can handle a wide range of contexts and styles.<br/><br/>\n",
    "\n",
    "    <b>Fine-tuning:</b><br/> \n",
    "    - The distribution patterns revealed in the charts should guide the fine-tuning process. Imbalanced categories may require techniques like weighted sampling or loss adjustment to ensure equal representation during fine-tuning.<br/>\n",
    "    - Columns with high semantic uniqueness could be especially useful for fine-tuning models on specific domains or tasks, as they likely contain a wide range of relevant examples.<br/>\n",
    "    - Consider the average word count per row when deciding on sequence length for transformer-based models during fine-tuning.<br/><br/>\n",
    "\n",
    "    <b>Designing/Iterating on Data to Fill Data Gaps:</b><br/> \n",
    "    - Analyze the distribution charts to identify underrepresented categories. These areas may require additional data collection or augmentation to ensure comprehensive model performance.<br/>\n",
    "    - If certain text diversity scores are low, consider ways to introduce more variety in those columns, either through data augmentation techniques or targeted data collection.<br/>\n",
    "    - For columns with very high cardinality, consider if grouping or categorization might be beneficial to prevent overfitting on rare categories.<br/>\n",
    "    - If semantic uniqueness is low in certain areas, it might indicate a need for more diverse examples in those categories to improve model generalization.<br/><br/>\n",
    "\n",
    "    <b>General Considerations:</b><br/> \n",
    "    - The overall uniqueness of the dataset impacts models that require diverse examples. However, care should be taken to address any imbalances revealed in the distribution charts.<br/>\n",
    "    - Monitor for potential biases in the data that could be propagated or amplified by machine learning models.<br/>\n",
    "    - Consider privacy implications, especially for high-cardinality columns that might contain identifiable information.<br/>\n",
    "    - The text complexity (average words per row) should inform decisions about model architecture and preprocessing steps.<br/><br/>\n",
    "    \"\"\"\n",
    "\n",
    "    story.append(Paragraph(conclusion_text, styles['BodyText']))\n",
    "\n",
    "    metric_definition_text = \"\"\"\n",
    "    This section provides definitions for the metrics used in the report.<br/><br/>\n",
    "    <b>Key Metrics</b><br/>\n",
    "    ‚Ä¢ <b>Unique Rows:</b> Percentage of rows that are unique in the dataset.<br/>\n",
    "    ‚Ä¢ <b>Semantically Unique Rows:</b> Percentage of rows that are semantically unique, based on TF-IDF.<br/>\n",
    "    ‚Ä¢ <b>Text Diversity:</b> Average Text Diversity Index (defined below) across all text columns, with higher values indicating more diverse content.<br/>\n",
    "    ‚Ä¢ <b>Gini-Simpson Diversity:</b> Average Gini-Simpson Index (defined below) across all categorical columns. Higher values indicating greater diversity.<br/><br/>\n",
    "\n",
    "    <b>Dataset Overview</b><br/>\n",
    "    The enhanced dataset overview provides key metrics about the structure, uniqueness, complexity, and quality of the data:<br/>\n",
    "    ‚Ä¢ <b>Number of Rows and Columns:</b> Indicates the size and dimensionality of the dataset.<br/>\n",
    "    ‚Ä¢ <b>Categorical and Numerical Columns:</b> Gives insight into the types of data present, helping to guide appropriate analysis techniques.<br/>\n",
    "    ‚Ä¢ <b>Data Completeness:</b> Shows the overall percentage of non-null values across all columns, indicating the dataset's overall quality and potential need for imputation.<br/>\n",
    "    ‚Ä¢ <b>Unique and Semantically Unique Rows:</b> Demonstrates the level of data diversity and potential redundancy in the dataset.<br/>\n",
    "    ‚Ä¢ <b>Average Words per Row:</b> Provides an indication of the typical complexity or detail level of each entry.<br/>\n",
    "    ‚Ä¢ <b>Average Tokens per Row and Total Tokens:</b> These metrics correspond to tokens used in Large Language Models (LLMs), giving an estimate of the dataset's complexity from an LLM processing perspective.<br/>\n",
    "    ‚Ä¢ <b>Average Text Diversity:</b> Average Text Diversity Index (defined below) across all text columns, with higher values indicating more diverse content.<br/>\n",
    "    ‚Ä¢ <b>Average Gini-Simpson Index:</b> Average Gini-Simpson Index (defined below) across all categorical columns. Higher values indicating greater diversity.<br/><br/>\n",
    "\n",
    "    <b>Dataset Schema & Preview</b><br/>\n",
    "    The schema table provides an overview of each column in the dataset, including the data type, the count of non-null and null values, and the average length (where applicable). This information is crucial for understanding the structure of the data and identifying potential data quality issues such as missing values or unexpected data types.<br/>\n",
    "    ‚Ä¢ <b>Data Type:</b> Categorical, Numeric, Text or Other. Categorical columns are those whose percentage of unique values are low; Text columns are non-Categorical columns with at least 2 spaces per Row, on average.<br/>\n",
    "    ‚Ä¢ <b>Total Count:</b> Total number of values in the Column.<br/>\n",
    "    ‚Ä¢ <b>% Null:</b> Percentage of null values in the Column.<br/>\n",
    "    ‚Ä¢ <b>Average Length:</b> Average character count of the values (for each text column).<br/>\n",
    "    ‚Ä¢ <b>Avg Tokens:</b> Average number of tokens in the values (for each text column).<br/><br/>\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if 'feature_cardinality' in data:\n",
    "        metric_definition_text += \"\"\"\n",
    "        <b>Column Cardinality</b><br/>\n",
    "        ‚Ä¢ <b>Column cardinality:</b> Represents the number of unique values for each column in the dataset. Higher cardinality indicates more diverse values within a Column.<br/><br/>\n",
    "\n",
    "        \"\"\"\n",
    "    \n",
    "    metric_definition_text += \"\"\"\n",
    "    <b>Column Distributions</b><br/>\n",
    "    Column distributions show the frequency of different values within each column. These visualizations help identify common patterns, imbalances, or biases in the data.<br/>\n",
    "    ‚Ä¢ <b>Pareto Chart:</b> A Pareto chart illustrates the distribution of domain in the dataset. The bars represent the count for each category, while the line shows the cumulative percentage. Only the top 75 categories are shown individually. The remaining categories are grouped as 'Other'. This visualization helps identify the most significant categories and their relative importance.<br/>\n",
    "    ‚Ä¢ <b>Gini-Simpson Index:</b> A diversity index for categorical columns. It quantifies the probability that two values taken at random from the column (with replacement) are different. Higher values indicate greater diversity.<br/>\n",
    "    ‚Ä¢ <b>Text Diversity Index:</b> A diversity index for text columns. It is defined as the average correlation between each row's TF-IDF vector and the dataset's TF-IDF matrix. Higher values indicate greater diversity.<br/><br/>\n",
    "    \"\"\"\n",
    "\n",
    "    story.append(Paragraph(\"Metric Definitions\", styles['Heading1']))\n",
    "    story.append(Paragraph(metric_definition_text, styles['BodyText']))\n",
    "\n",
    "    # Build the PDF\n",
    "    doc.build(story)\n",
    "    print(f\"PDF created: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report_pdf(results, dataset_df,  'data_quality_report.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# field = 'Example'\n",
    "# counts = [7, 7, 7, 10, 6, 6, 3, 1, 1, 2]\n",
    "# bins = [0.0, 27.7, 55.4, 83.1, 110.8, 138.5, 166.2, 193.9, 221.6, 249.29999999999998, 277.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "navhelpers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
