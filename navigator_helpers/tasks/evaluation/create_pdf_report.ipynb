{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a PDF of the Data Quality Report for Data Desginer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéõÔ∏è Import things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from navigator_helpers.llms.llm_suite import GretelLLMSuite\n",
    "from navigator_helpers.tasks.evaluation.evaluation import (\n",
    "    BaseEvaluationTaskSuite,\n",
    "    VisualizationTaskSuite\n",
    ")\n",
    "\n",
    "# set environment variable 'GRETEL_PROD_API_KEY' from https://console.gretel.ai/users/me/key\n",
    "gretel_prod_api_key = input(\"Enter your Gretel API key from https://console.gretel.ai/users/me/key: \")\n",
    "os.environ['GRETEL_PROD_API_KEY'] = gretel_prod_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¢ Choose Dataset for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of samples to load from the dataset for testing\n",
    "# Set to None to use the full dataset\n",
    "NUM_SAMPLES = 1000\n",
    "\n",
    "datasets_dict = {\n",
    "    \"synthetic_text_to_sql\": {\n",
    "        \"dataset_kwargs\": {\n",
    "            \"path\": \"gretelai/synthetic_text_to_sql\",\n",
    "            \"split\": \"train\"\n",
    "        },\n",
    "        \"code_lang\": \"sql\",\n",
    "        \"eval_kwargs\":{\n",
    "            \"instruction_col_name\": \"sql_prompt\",\n",
    "            \"code_col_name\": \"sql\",\n",
    "            \"context_col_name\": \"sql_context\"\n",
    "        }\n",
    "    },\n",
    "    \"gsm8k\": {\n",
    "        \"dataset_kwargs\": {\n",
    "            \"path\": \"openai/gsm8k\",\n",
    "            \"name\": \"main\",\n",
    "            \"split\": \"train\"\n",
    "        },\n",
    "        \"eval_kwargs\": {\n",
    "            \"instruction_col_name\": \"question\",\n",
    "            \"code_col_name\": \"answer\",\n",
    "        }\n",
    "    },\n",
    "    \"synthetic_gsm8k\": {\n",
    "        \"dataset_kwargs\": {\n",
    "            \"path\": \"gretelai/synthetic-gsm8k-reflection-405b\",\n",
    "            \"split\": \"train\"\n",
    "        },\n",
    "        \"eval_kwargs\": {\n",
    "            \"instruction_col_name\": \"question\",\n",
    "            \"code_col_name\": \"answer\",\n",
    "        }\n",
    "    },\n",
    "    \"xlcost_text_to_code\": {\n",
    "        \"dataset_kwargs\": {\n",
    "            \"path\": \"codeparrot/xlcost-text-to-code\",\n",
    "            \"split\": \"train\"\n",
    "        },\n",
    "        \"code_lang\": \"python\",\n",
    "        \"eval_kwargs\": {\n",
    "            \"instruction_col_name\": \"text\",\n",
    "            \"code_col_name\": \"code\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# Prompt user to select a dataset\n",
    "print(\"Available datasets:\")\n",
    "for key in datasets_dict.keys():\n",
    "    print(f\" - {key}\")\n",
    "\n",
    "selected_dataset = input(\"\\nEnter the name of the dataset to load: \").strip()\n",
    "\n",
    "# Load the selected dataset\n",
    "if selected_dataset in datasets_dict:\n",
    "    dataset_dict = datasets_dict[selected_dataset]\n",
    "    eval_kwargs = dataset_dict[\"eval_kwargs\"]\n",
    "    code_lang = dataset_dict[\"code_lang\"] if \"code_lang\" in dataset_dict.keys() else None\n",
    "    dataset = load_dataset(**dataset_dict[\"dataset_kwargs\"])\n",
    "\n",
    "    # Optionally, select a subset if NUM_SAMPLES is specified\n",
    "    if NUM_SAMPLES is not None and NUM_SAMPLES < len(dataset):\n",
    "        dataset = dataset.select(range(NUM_SAMPLES))\n",
    "    \n",
    "    dataset_df = dataset.to_pandas()\n",
    "    \n",
    "    print(f\"Loaded dataset '{selected_dataset}' successfully!\")\n",
    "else:\n",
    "    print(\"Error: Dataset not found. Please enter a valid dataset name.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'results': {'row_uniqueness': {'percent_unique': 100.0,\n",
    "   'percent_semantically_unique': 100.0,\n",
    "   'non_unique_ids': [],\n",
    "   'non_semantically_unique_ids': []},\n",
    "  'feature_cardinality': {'id': 10,\n",
    "   'domain': 5,\n",
    "   'topic': 10,\n",
    "   'complexity': 4,\n",
    "   'prompt': 10,\n",
    "   'dependency_list': 3,\n",
    "   'code': 10,\n",
    "   'code_is_valid': 2,\n",
    "   'code_score': 9,\n",
    "   'code_severity': 3,\n",
    "   'code_messages': 9,\n",
    "   'llm_judge_scores': 10},\n",
    "  'feature_distribution': {'distribution': {'id': None,\n",
    "    'domain': {'avg_length': 19.8,\n",
    "     'std_length': 1.9321835661585918,\n",
    "     'avg_word_count': 2.1,\n",
    "     'word_count_histogram': ([0, 0, 0, 1, 0, 0, 7, 0, 0, 2],\n",
    "      [0.0,\n",
    "       0.3,\n",
    "       0.6,\n",
    "       0.8999999999999999,\n",
    "       1.2,\n",
    "       1.5,\n",
    "       1.7999999999999998,\n",
    "       2.1,\n",
    "       2.4,\n",
    "       2.6999999999999997,\n",
    "       3.0])},\n",
    "    'topic': {'avg_length': 20.4,\n",
    "     'std_length': 4.033195589934446,\n",
    "     'avg_word_count': 2.2,\n",
    "     'word_count_histogram': ([0, 0, 0, 0, 0, 0, 8, 0, 0, 2],\n",
    "      [0.0,\n",
    "       0.3,\n",
    "       0.6,\n",
    "       0.8999999999999999,\n",
    "       1.2,\n",
    "       1.5,\n",
    "       1.7999999999999998,\n",
    "       2.1,\n",
    "       2.4,\n",
    "       2.6999999999999997,\n",
    "       3.0])},\n",
    "    'complexity': {'avg_length': 57.9,\n",
    "     'std_length': 3.8427420765212266,\n",
    "     'avg_word_count': 6.6,\n",
    "     'word_count_histogram': ([0, 0, 0, 0, 0, 0, 0, 7, 0, 3],\n",
    "      [0.0,\n",
    "       0.8,\n",
    "       1.6,\n",
    "       2.4000000000000004,\n",
    "       3.2,\n",
    "       4.0,\n",
    "       4.800000000000001,\n",
    "       5.6000000000000005,\n",
    "       6.4,\n",
    "       7.2,\n",
    "       8.0])},\n",
    "    'prompt': {'avg_length': 1169.1,\n",
    "     'std_length': 417.28126672012917,\n",
    "     'avg_word_count': 174.1,\n",
    "     'word_count_histogram': ([0, 0, 0, 6, 2, 1, 0, 0, 0, 1],\n",
    "      [0.0,\n",
    "       38.3,\n",
    "       76.6,\n",
    "       114.89999999999999,\n",
    "       153.2,\n",
    "       191.5,\n",
    "       229.79999999999998,\n",
    "       268.09999999999997,\n",
    "       306.4,\n",
    "       344.7,\n",
    "       383.0])},\n",
    "    'dependency_list': {\"['matplotlib', 'numpy', 'pandas', 'scikit-learn', 'tensorflow']\": 5,\n",
    "     \"['matplotlib', 'numpy', 'pandas', 'scikit-learn', 'seaborn']\": 4,\n",
    "     \"['matplotlib', 'numpy', 'pandas', 'seaborn', 'sklearn']\": 1},\n",
    "    'code': {'avg_length': 1211.5,\n",
    "     'std_length': 744.9386626502286,\n",
    "     'avg_word_count': 119.6,\n",
    "     'word_count_histogram': ([1, 1, 1, 2, 1, 2, 0, 1, 0, 1],\n",
    "      [0.0,\n",
    "       27.0,\n",
    "       54.0,\n",
    "       81.0,\n",
    "       108.0,\n",
    "       135.0,\n",
    "       162.0,\n",
    "       189.0,\n",
    "       216.0,\n",
    "       243.0,\n",
    "       270.0])},\n",
    "    'code_is_valid': {True: 8, False: 2},\n",
    "    'code_score': {10.0: 2,\n",
    "     7.5: 1,\n",
    "     8.333333333333334: 1,\n",
    "     8.88888888888889: 1,\n",
    "     5.9259259259259265: 1,\n",
    "     7.948717948717949: 1,\n",
    "     6.938775510204081: 1,\n",
    "     8.5: 1,\n",
    "     9.0: 1},\n",
    "    'code_severity': {'warning': 6, 'none': 2, 'error': 2},\n",
    "    'code_messages': {'avg_length': 376.8,\n",
    "     'std_length': 340.17211329952767,\n",
    "     'avg_word_count': 38.1,\n",
    "     'word_count_histogram': ([2, 1, 1, 3, 2, 0, 0, 0, 0, 1],\n",
    "      [0.0,\n",
    "       11.8,\n",
    "       23.6,\n",
    "       35.400000000000006,\n",
    "       47.2,\n",
    "       59.0,\n",
    "       70.80000000000001,\n",
    "       82.60000000000001,\n",
    "       94.4,\n",
    "       106.2,\n",
    "       118.0])},\n",
    "    'llm_judge_scores': {'avg_length': 1067.8,\n",
    "     'std_length': 186.9859174733042,\n",
    "     'avg_word_count': 141.3,\n",
    "     'word_count_histogram': ([0, 0, 0, 0, 0, 1, 2, 0, 5, 2],\n",
    "      [0.0,\n",
    "       17.6,\n",
    "       35.2,\n",
    "       52.800000000000004,\n",
    "       70.4,\n",
    "       88.0,\n",
    "       105.60000000000001,\n",
    "       123.20000000000002,\n",
    "       140.8,\n",
    "       158.4,\n",
    "       176.0])}},\n",
    "   'score': {'id': None,\n",
    "    'domain': {'text_diversity_index': 0.47491147384936305},\n",
    "    'topic': {'text_diversity_index': 0.6837722339831621},\n",
    "    'complexity': {'text_diversity_index': 0.4545170585682964},\n",
    "    'prompt': {'text_diversity_index': 0.4155945214318475},\n",
    "    'dependency_list': {'gini-simpson_index': 0.58},\n",
    "    'code': {'text_diversity_index': 0.6259925995996702},\n",
    "    'code_is_valid': {'gini-simpson_index': 0.32},\n",
    "    'code_score': {'gini-simpson_index': 0.88},\n",
    "    'code_severity': {'gini-simpson_index': 0.56},\n",
    "    'code_messages': {'text_diversity_index': 0.4101459511937783},\n",
    "    'llm_judge_scores': {'text_diversity_index': 0.20102246878316166}}},\n",
    "  'num_words_per_record': {'average_words_per_record': 44.545454545454554,\n",
    "   'word_counts_per_column': {'domain': 2.1,\n",
    "    'topic': 2.2,\n",
    "    'complexity': 6.6,\n",
    "    'prompt': 174.1,\n",
    "    'dependency_list': 5.0,\n",
    "    'code': 119.6,\n",
    "    'code_is_valid': 0.0,\n",
    "    'code_score': 0.0,\n",
    "    'code_severity': 1.0,\n",
    "    'code_messages': 38.1,\n",
    "    'llm_judge_scores': 141.3},\n",
    "   'average_tokens_per_record': 920.6,\n",
    "   'tokens_per_column': {'id': 1.0,\n",
    "    'domain': 2.5,\n",
    "    'topic': 2.6,\n",
    "    'complexity': 10.9,\n",
    "    'prompt': 245.7,\n",
    "    'dependency_list': 19.8,\n",
    "    'code': 281.2,\n",
    "    'code_is_valid': 1.0,\n",
    "    'code_score': 5.1,\n",
    "    'code_severity': 1.0,\n",
    "    'code_messages': 119.3,\n",
    "    'llm_judge_scores': 230.4},\n",
    "   'total_tokens': 9206},\n",
    "  'column_notes': {'id': 'Unique ID',\n",
    "   'domain': 'Seed Column',\n",
    "   'topic': 'Seed Column',\n",
    "   'complexity': 'Seed Column',\n",
    "   'prompt': '',\n",
    "   'dependency_list': 'Seed Column',\n",
    "   'code': '',\n",
    "   'code_is_valid': 'Post Processing Column',\n",
    "   'code_score': 'Post Processing Column',\n",
    "   'code_severity': 'Post Processing Column',\n",
    "   'code_messages': 'Post Processing Column',\n",
    "   'llm_judge_scores': 'Post Processing Column'},\n",
    "  'column_data_types': {'id': 'Other',\n",
    "   'domain': 'Text',\n",
    "   'topic': 'Text',\n",
    "   'complexity': 'Text',\n",
    "   'prompt': 'Text',\n",
    "   'dependency_list': 'Categorical',\n",
    "   'code': 'Text',\n",
    "   'code_is_valid': 'Categorical',\n",
    "   'code_score': 'Categorical',\n",
    "   'code_severity': 'Categorical',\n",
    "   'code_messages': 'Text',\n",
    "   'llm_judge_scores': 'Text'},\n",
    "  'llm_as_a_judge_mean_scores': {'relevance_score': 4.0,\n",
    "   'correctness_score': 3.8,\n",
    "   'readability_score': 3.0,\n",
    "   'efficiency_score': 3.0,\n",
    "   'pythonic_score': 3.0},\n",
    "  'valid_records_score': {'count': 8, 'percent': 0.8}},\n",
    " 'dataset_overview_statistics': {'number_of_rows': 10,\n",
    "  'number_of_columns': 12,\n",
    "  'number_of_categorical_columns': 3,\n",
    "  'number_of_text_columns': 8,\n",
    "  'number_of_numerical_columns': 0,\n",
    "  'number_of_other_columns': 1,\n",
    "  'number_of_seed_columns': 4,\n",
    "  'data_completeness': 100.0,\n",
    "  'single_row': {'id': 482,\n",
    "   'domain': 'Financial Services',\n",
    "   'topic': 'Portfolio Management',\n",
    "   'complexity': 'Beginner: Basic syntax, data types, and control structures',\n",
    "   'prompt': 'Write a Python function named \\'calculate_portfolio_value\\' that takes two arguments: \\'stocks\\' and \\'crypto\\'. Here, \\'stocks\\' is a dictionary where each key is a stock symbol and the corresponding value is the number of shares owned. \\'Crypto\\' is another dictionary where each key is a cryptocurrency symbol and the corresponding value is the number of coins owned.\\n\\nThe function should return the total value of the portfolio, assuming that the current price of each stock and cryptocurrency is available as a global variable. For simplicity, you can assume that the current price of each stock and cryptocurrency is a float. The total value of the portfolio is the sum of the value of all stocks and cryptocurrencies owned.\\n\\nThis function will be used in a portfolio management context in the financial services domain. It is a beginner level task, requiring only basic syntax, data types, and control structures.\\n\\n### Instructions\\n    * The code should have a complexity of \"Beginner: Basic syntax, data types, and control structures\".\\n    * Write code that might be used in the \"Financial Services\" industry within a \"Portfolio Management\" context.\\n    * Try to include at least 1 of the following Python packages:  `pandas`, `matplotlib`, `numpy`, `scikit-learn`, `seaborn`.\\n    * Include only the code, without any comments or additional text.\\n',\n",
    "   'dependency_list': ['pandas',\n",
    "    'matplotlib',\n",
    "    'numpy',\n",
    "    'scikit-learn',\n",
    "    'seaborn'],\n",
    "   'code': \"import numpy as np\\n\\nstocks = {'AAPL': 10, 'GOOG': 5, 'AMZN': 2}\\ncrypto = {'BTC': 0.5, 'ETH': 2.0}\\n\\nstock_prices = {'AAPL': 150.25, 'GOOG': 1200.0, 'AMZN': 2000.0}\\ncrypto_prices = {'BTC': 30000.0, 'ETH': 1500.0}\\n\\ndef calculate_portfolio_value(stocks, crypto):\\n    total_value = 0.0\\n\\n    for stock, shares in stocks.items():\\n        total_value += stock_prices[stock] * shares\\n\\n    for coin, amount in crypto.items():\\n        total_value += crypto_prices[coin] * amount\\n\\n    return total_value\",\n",
    "   'code_is_valid': True,\n",
    "   'code_score': 7.5,\n",
    "   'code_severity': 'warning',\n",
    "   'code_messages': [{'symbol': 'redefined-outer-name',\n",
    "     'msg': \"Redefining name 'stocks' from outer scope (line 3)\",\n",
    "     'category': 'warning',\n",
    "     'line': 9,\n",
    "     'column': 30},\n",
    "    {'symbol': 'redefined-outer-name',\n",
    "     'msg': \"Redefining name 'crypto' from outer scope (line 4)\",\n",
    "     'category': 'warning',\n",
    "     'line': 9,\n",
    "     'column': 38},\n",
    "    {'symbol': 'unused-import',\n",
    "     'msg': 'Unused numpy imported as np',\n",
    "     'category': 'warning',\n",
    "     'line': 1,\n",
    "     'column': 0}],\n",
    "   'llm_judge_scores': '{\\n    \"relevance\": {\"score\": 4, \"reasoning\": \"The code perfectly meets all specified requirements, including the use of the numpy package.\"},\\n    \"correctness\": {\"score\": 4, \"reasoning\": \"The code executes flawlessly, producing accurate and complete results as per the requirement.\"},\\n    \"readability\": {\"score\": 3, \"reasoning\": \"The code is well-formatted and commented, making it relatively easy to understand and maintain.\"},\\n    \"efficiency\": {\"score\": 3, \"reasoning\": \"The solution is efficient, with good use of Python\\'s built-in functions and libraries.\"},\\n    \"pythonic\": {\"score\": 3, \"reasoning\": \"The code closely follows Python conventions and adheres to many best practices.\"}\\n}'},\n",
    "  'dtypes': {'id': 'int64',\n",
    "   'domain': 'object',\n",
    "   'topic': 'object',\n",
    "   'complexity': 'object',\n",
    "   'prompt': 'object',\n",
    "   'dependency_list': 'object',\n",
    "   'code': 'object',\n",
    "   'code_is_valid': 'bool',\n",
    "   'code_score': 'float64',\n",
    "   'code_severity': 'object',\n",
    "   'code_messages': 'object',\n",
    "   'llm_judge_scores': 'object'},\n",
    "  'counts_per_column': {'id': 10,\n",
    "   'domain': 10,\n",
    "   'topic': 10,\n",
    "   'complexity': 10,\n",
    "   'prompt': 10,\n",
    "   'dependency_list': 10,\n",
    "   'code': 10,\n",
    "   'code_is_valid': 10,\n",
    "   'code_score': 10,\n",
    "   'code_severity': 10,\n",
    "   'code_messages': 10,\n",
    "   'llm_judge_scores': 10},\n",
    "  'percent_null_per_column': {'id': 0.0,\n",
    "   'domain': 0.0,\n",
    "   'topic': 0.0,\n",
    "   'complexity': 0.0,\n",
    "   'prompt': 0.0,\n",
    "   'dependency_list': 0.0,\n",
    "   'code': 0.0,\n",
    "   'code_is_valid': 0.0,\n",
    "   'code_score': 0.0,\n",
    "   'code_severity': 0.0,\n",
    "   'code_messages': 0.0,\n",
    "   'llm_judge_scores': 0.0},\n",
    "  'seed_columns_preview': [{'domain': 'Financial Services',\n",
    "    'topic': 'Portfolio Management',\n",
    "    'complexity': 'Beginner: Basic syntax, data types, and control structures',\n",
    "    'dependency_list': ['pandas',\n",
    "     'matplotlib',\n",
    "     'numpy',\n",
    "     'scikit-learn',\n",
    "     'seaborn']},\n",
    "   {'domain': 'Telecommunications',\n",
    "    'topic': 'Mobile Networks',\n",
    "    'complexity': 'Intermediate: Functions, modules, and file handling',\n",
    "    'dependency_list': ['pandas',\n",
    "     'scikit-learn',\n",
    "     'matplotlib',\n",
    "     'seaborn',\n",
    "     'numpy']},\n",
    "   {'domain': 'Financial Services',\n",
    "    'topic': 'Loan Processing',\n",
    "    'complexity': 'Beginner: Basic syntax, data types, and control structures',\n",
    "    'dependency_list': ['numpy',\n",
    "     'pandas',\n",
    "     'tensorflow',\n",
    "     'matplotlib',\n",
    "     'scikit-learn']},\n",
    "   {'domain': 'Financial Services',\n",
    "    'topic': 'Insurance Underwriting',\n",
    "    'complexity': 'Advanced: Object-oriented programming and exception handling',\n",
    "    'dependency_list': ['seaborn',\n",
    "     'scikit-learn',\n",
    "     'numpy',\n",
    "     'pandas',\n",
    "     'matplotlib']},\n",
    "   {'domain': 'Healthcare Technology',\n",
    "    'topic': 'Health Information Exchange',\n",
    "    'complexity': 'Beginner: Basic syntax, data types, and control structures',\n",
    "    'dependency_list': ['matplotlib',\n",
    "     'scikit-learn',\n",
    "     'tensorflow',\n",
    "     'numpy',\n",
    "     'pandas']}]}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_suite = GretelLLMSuite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a dictionary to store evaluation results\n",
    "# results = {}\n",
    "\n",
    "# # Uncomment the following lines to run individual evaluation tasks\n",
    "# results.update({\"row_uniqueness\": BaseEvaluationTaskSuite(llm_suite, dataset_df).row_uniqueness()})\n",
    "# results.update({\"feature_cardinality\": BaseEvaluationTaskSuite(llm_suite, dataset_df).feature_cardinality()})\n",
    "# results.update({\"feature_distribution\": BaseEvaluationTaskSuite(llm_suite, dataset_df).feature_distribution()})\n",
    "# results.update({\"num_words_per_record\": BaseEvaluationTaskSuite(llm_suite, dataset_df).num_words_per_record()})\n",
    "\n",
    "# # Uncomment this line to run everything, including LLM-as-a-judge\n",
    "# # results = BaseEvaluationTaskSuite(llm_suite, dataset_df, code_lang, eval_kwargs).evaluate_all()\n",
    "\n",
    "# pprint(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from typing import List, Optional, Tuple, Union, Dict, Any\n",
    "import math\n",
    "import io\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.io import to_image\n",
    "from PIL import Image as PILImage\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, PageBreak, Paragraph, Spacer, Image, Table, TableStyle\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.lib.enums import TA_CENTER\n",
    "\n",
    "# Constants\n",
    "FIG_WIDTH = 7  # inches\n",
    "FIG_HEIGHT = 3.6  # inches\n",
    "SCORE_VALUES = [\n",
    "    {\"label\": \"Very poor\", \"color\": \"rgb(229, 60, 26)\"},\n",
    "    {\"label\": \"Poor\", \"color\": \"rgb(229, 128, 26)\"},\n",
    "    {\"label\": \"Average\", \"color\": \"rgb(229, 161, 26)\"},\n",
    "    {\"label\": \"Good\", \"color\": \"rgb(183, 210, 45)\"},\n",
    "    {\"label\": \"Excellent\", \"color\": \"rgb(72, 210, 45)\"},\n",
    "]\n",
    "PRIMARY_PALETTE = ['#2E1065', '#D3A66E', '#110420', '#4F00A9', '#F9EFDE', '#1D0B32', '#8D32FA', '#C399FF', '#EFE5FF', '#EFD7AD', '#F4E3C6', '#FBF7ED', '#A59DAD', '#D2CED6', '#E8E7EB']\n",
    "SECONDARY_PALETTE = ['#052095', '#FF6BA9', '#3056F2', '#FFA8CC', '#8BB9FF', '#FFEDF5', '#E5F0FF', '#1E9C98', '#92F6F4', '#C5FEFF', '#E8FEFF', '#FF9248', '#FFB38A', '#FFD7B5', '#FFECDC', '#FF6700', '#FFCA1A', '#FFE16D', '#FFF099', '#FFFDE3', '#ECA10A']\n",
    "\n",
    "# Set up custom color palette for seaborn\n",
    "sns.set_theme(style=\"white\")\n",
    "sns.set_palette(sns.color_palette(SECONDARY_PALETTE))\n",
    "\n",
    "def create_chart(data: pd.Series, title: str, xlabel: str, ylabel: str) -> Image:\n",
    "    fig, ax = plt.subplots(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "    bars = ax.bar(range(len(data)), data.values, color='#4F00A9')\n",
    "    ax.set_facecolor('white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    ax.set_title(title, fontsize=10, color='#1D0B32')\n",
    "    ax.set_xlabel(xlabel, fontsize=10, color='#1D0B32')\n",
    "    ax.set_ylabel(ylabel, fontsize=10, color='#1D0B32')\n",
    "    ax.set_xticks(range(len(data)))\n",
    "    \n",
    "    truncated_labels = [str(label)[:17] + '...' if len(str(label)) > 20 else str(label) for label in data.index]\n",
    "    ax.set_xticklabels(truncated_labels, rotation=45, ha='right', fontsize=8, color='#1D0B32')\n",
    "    \n",
    "    ax.tick_params(axis='both', colors='#1D0B32')\n",
    "    ax.tick_params(axis='y', labelsize=6)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}' if isinstance(height, float) else f'{height}',\n",
    "                ha='center', va='bottom', fontsize=8, color='#1D0B32')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    img_buffer = io.BytesIO()\n",
    "    plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')\n",
    "    img_buffer.seek(0)\n",
    "    return Image(img_buffer, width=7*inch, height=4*inch)\n",
    "\n",
    "def create_pareto_chart(data: pd.DataFrame, title: str) -> Image:\n",
    "    fig, ax1 = plt.subplots(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    bars = ax1.bar(range(len(data)), data['count'], color='#4F00A9')\n",
    "    ax1.set_facecolor('white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    ax1.set_xlabel('Categories', fontsize=10, color='#1D0B32')\n",
    "    ax1.set_ylabel('Count', fontsize=10, color='#1D0B32')\n",
    "    ax1.set_title(title, fontsize=10, color='#1D0B32')\n",
    "    \n",
    "    cumulative_percentage = 100 * data['count'].cumsum() / data['count'].sum()\n",
    "    ax2.plot(range(len(data)), cumulative_percentage, color='#FF6700', marker='D', ms=4)\n",
    "    ax2.set_ylabel('Cumulative Percentage', fontsize=10, color='#1D0B32')\n",
    "    ax2.set_ylim([0, 110])\n",
    "    \n",
    "    ax1.tick_params(axis='both', colors='#1D0B32')\n",
    "    ax2.tick_params(axis='both', colors='#1D0B32')\n",
    "    ax1.tick_params(axis='y', labelsize=6)\n",
    "    ax2.tick_params(axis='y', labelsize=6)\n",
    "    \n",
    "    ax1.set_xticks(range(len(data)))\n",
    "    truncated_labels = [str(label)[:17] + '...' if len(str(label)) > 20 else str(label) for label in data.index]\n",
    "    ax1.set_xticklabels(truncated_labels, rotation=45, ha='right', fontsize=6, color='#1D0B32')\n",
    "    \n",
    "    for i, v in enumerate(data['count']):\n",
    "        ax1.text(i, v, f'{v:.2f}' if isinstance(v, float) else f'{v}', ha='center', va='bottom', fontsize=8, color='#1D0B32')\n",
    "    \n",
    "    ax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0f}%'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    img_buffer = io.BytesIO()\n",
    "    plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')\n",
    "    img_buffer.seek(0)\n",
    "    return Image(img_buffer, width=FIG_WIDTH*inch, height=FIG_HEIGHT*inch)\n",
    "\n",
    "def create_text_diversity_chart(text_diversity_df: pd.DataFrame) -> Image:\n",
    "    plt.figure(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "    ax = sns.barplot(x=text_diversity_df.index, y='diversity_index', data=text_diversity_df, color='#4F00A9')\n",
    "    ax.set_facecolor('white')\n",
    "    plt.gcf().patch.set_facecolor('white')\n",
    "    \n",
    "    plt.title(\"Text Diversity Indices\", fontsize=10, color='#1D0B32')\n",
    "    plt.ylabel(\"Diversity Index\", fontsize=10, color='#1D0B32')\n",
    "    plt.xlabel(\"\", fontsize=10, color='#1D0B32')\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=8, color='#1D0B32')\n",
    "    \n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    ax.tick_params(axis='both', colors='#1D0B32')\n",
    "    ax.tick_params(axis='y', labelsize=6)\n",
    "    \n",
    "    for i, v in enumerate(text_diversity_df['diversity_index']):\n",
    "        ax.text(i, v, f'{v:.2f}', ha='center', va='bottom', fontsize=8, color='#1D0B32')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    img_buffer = io.BytesIO()\n",
    "    plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')\n",
    "    img_buffer.seek(0)\n",
    "    return Image(img_buffer, width=FIG_WIDTH*inch, height=FIG_HEIGHT*inch)\n",
    "\n",
    "def create_histogram(counts: List[int], bins: List[float], col_name: str, data_type: str = \"Text\") -> Image:\n",
    "    assert data_type in [\"Text\", \"Numeric\"], f\"Invalid data type: {data_type}\"\n",
    "    if data_type == \"Text\":\n",
    "        x_label = \"Word Count\"\n",
    "    else:\n",
    "        x_label = \"Value\"\n",
    "    \n",
    "    plt.figure(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "    plt.hist(bins[:-1], bins, weights=counts, color='#4F00A9')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(x_label, fontsize=10, color='#1D0B32')\n",
    "    plt.ylabel(\"Count\", fontsize=10, color='#1D0B32')\n",
    "    plt.title(f\"{col_name.replace('_', ' ').title()}: {x_label} Distribution (Histogram)\", fontsize=10, color='#1D0B32')\n",
    "    plt.xticks(fontsize=6, color='#1D0B32')\n",
    "    plt.yticks(fontsize=6, color='#1D0B32')\n",
    "\n",
    "    # Add counts above bars\n",
    "    for i in range(len(counts)):\n",
    "        plt.text((bins[i]+bins[i+1])/2, counts[i], str(counts[i]), ha='center', va='bottom', fontsize=8, color='#1D0B32')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    img_buffer = io.BytesIO()\n",
    "    plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')\n",
    "    img_buffer.seek(0)\n",
    "    return Image(img_buffer, width=FIG_WIDTH*inch, height=FIG_HEIGHT*inch)\n",
    "\n",
    "def create_schema_table(data: Dict[str, Any]) -> Tuple[Table, Dict[str, float]]:\n",
    "    schema_data = [['Column Name', 'Type', 'Total Count', '% Null', 'Average Length', 'Avg Tokens', 'Note']]\n",
    "    dataset_columns = data['dataset_overview_statistics']['counts_per_column'].keys()\n",
    "    for col in dataset_columns:\n",
    "        dtype = data['dataset_overview_statistics']['dtypes'].get(col, '')\n",
    "        total_count = data['dataset_overview_statistics']['counts_per_column'].get(col, 0)\n",
    "        pcnt_null = f\"{data['dataset_overview_statistics']['percent_null_per_column'].get(col, 0) * 100 :.2f}%\"\n",
    "        avg_length = 'N/A'\n",
    "        avg_tokens = 'N/A'\n",
    "        if 'num_words_per_record' in data['results']:\n",
    "            num_words = data['results']['num_words_per_record']\n",
    "            if col in num_words['word_counts_per_column']:\n",
    "                avg_length = num_words['word_counts_per_column'][col]\n",
    "            if col in num_words['tokens_per_column']:\n",
    "                avg_tokens = num_words['tokens_per_column'][col]\n",
    "        if 'column_notes' in data['results']:\n",
    "            note = data['results']['column_notes'].get(col, '')\n",
    "        schema_data.append([col, dtype, total_count, f\"{pcnt_null}%\", avg_length, avg_tokens, note])\n",
    "    \n",
    "    table = Table(schema_data)\n",
    "    style = TableStyle([\n",
    "        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#4F00A9')),\n",
    "        ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),\n",
    "        ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
    "        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "        ('FONTSIZE', (0, 0), (-1, 0), 8),\n",
    "        ('BOTTOMPADDING', (0, 0), (-1, 0), 6),\n",
    "        ('BACKGROUND', (0, 1), (-1, -1), colors.HexColor('#EFE5FF')),\n",
    "        ('TEXTCOLOR', (0, 1), (-1, -1), colors.HexColor('#110420')),\n",
    "        ('ALIGN', (0, 1), (-1, -1), 'LEFT'),\n",
    "        ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),\n",
    "        ('FONTSIZE', (0, 1), (-1, -1), 7),\n",
    "        ('TOPPADDING', (0, 1), (-1, -1), 3),\n",
    "        ('BOTTOMPADDING', (0, 1), (-1, -1), 3),\n",
    "        ('GRID', (0, 0), (-1, -1), 1, colors.HexColor('#4F00A9'))\n",
    "    ])\n",
    "    table.setStyle(style)\n",
    "    return table\n",
    "\n",
    "def create_overview_table(overview_data: List[List[str]]) -> Table:\n",
    "    table = Table(overview_data, colWidths=[1.5*inch, 1.5*inch])\n",
    "    style = TableStyle([\n",
    "        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#4F00A9')),\n",
    "        ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),\n",
    "        ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
    "        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "        ('FONTSIZE', (0, 0), (-1, 0), 8),\n",
    "        ('BOTTOMPADDING', (0, 0), (-1, 0), 3),  # Reduced padding\n",
    "        ('BACKGROUND', (0, 1), (-1, -1), colors.HexColor('#EFE5FF')),\n",
    "        ('TEXTCOLOR', (0, 1), (-1, -1), colors.HexColor('#110420')),\n",
    "        ('ALIGN', (0, 1), (-1, -1), 'LEFT'),\n",
    "        ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),\n",
    "        ('FONTSIZE', (0, 1), (-1, -1), 7),\n",
    "        ('TOPPADDING', (0, 1), (-1, -1), 3),  # Minimal top padding\n",
    "        ('BOTTOMPADDING', (0, 1), (-1, -1), 3),  # Minimal bottom padding\n",
    "        ('GRID', (0, 0), (-1, -1), 0.5, colors.HexColor('#4F00A9'))  # Thinner grid lines\n",
    "    ])\n",
    "    table.setStyle(style)\n",
    "    return table\n",
    "\n",
    "def create_single_record_preview(row: Dict[str, Any]) -> str:\n",
    "    preview_text = \"\"\n",
    "    for column, value in row.items():\n",
    "        truncated_value = str(value)[:100] + ('...' if len(str(value)) > 100 else '')\n",
    "        preview_text += f\"<b>{column}:</b>\\t{truncated_value}\"\n",
    "        preview_text += \"<br/>\"\n",
    "    return preview_text\n",
    "\n",
    "def _generate_pointer_path(score: int) -> str:\n",
    "    theta = score * (282 - 34) / 100 - 34\n",
    "    rads = math.radians(theta)\n",
    "    radius = 0.45\n",
    "    size = 0.025\n",
    "    x1 = -1 * radius * math.cos(rads) + 0.5\n",
    "    y1 = radius * math.sin(rads) + 0.5\n",
    "    return f\"\"\"\n",
    "    M {x1} {y1}\n",
    "    L {-1 * size * math.cos(math.radians(theta - 90)) + 0.5}\n",
    "        {size * math.sin(math.radians(theta - 90)) + 0.5}\n",
    "    L {-1 * size * math.cos(math.radians(theta + 90)) + 0.5}\n",
    "        {size * math.sin(math.radians(theta + 90)) + 0.5}\n",
    "    Z\"\"\"\n",
    "\n",
    "def gauge_and_needle_chart(score: Optional[int], display_score: bool = True, marker_colors: Optional[List[str]] = None) -> go.Figure:\n",
    "    if score is None:\n",
    "        fig = go.Figure(\n",
    "            layout=go.Layout(\n",
    "                annotations=[\n",
    "                    go.layout.Annotation(\n",
    "                        text=\"N/A\",\n",
    "                        font=dict(color=\"rgba(174, 95, 5, 1)\", size=18),\n",
    "                        showarrow=False,\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\",\n",
    "                        x=0.5,\n",
    "                        y=0.5,\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        marker_colors = [\"rgb(220, 220, 220)\", \"rgba(255, 255, 255, 0)\"]\n",
    "        pie_values = [70, 30]\n",
    "    else:\n",
    "        if not marker_colors:\n",
    "            marker_colors = [s[\"color\"] for s in SCORE_VALUES]\n",
    "        if marker_colors[-1] != \"rgba(255, 255, 255, 0)\":\n",
    "            marker_colors.append(\"rgba(255, 255, 255, 0)\")\n",
    "        pie_values = [70 // (len(marker_colors) - 1)] * (len(marker_colors) - 1)\n",
    "        pie_values.append(30)\n",
    "        fig = go.Figure()\n",
    "\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        showlegend=False,\n",
    "        xaxis=dict(visible=False),\n",
    "        yaxis=dict(visible=False),\n",
    "        height=180,\n",
    "        width=180,\n",
    "        margin=dict(l=0, r=0, t=0, b=0),\n",
    "        paper_bgcolor=\"rgba(0,0,0,0)\",\n",
    "        hovermode=False,\n",
    "        modebar=None,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Pie(\n",
    "            name=\"gauge\",\n",
    "            values=pie_values,\n",
    "            marker=dict(\n",
    "                colors=marker_colors,\n",
    "                line=dict(width=4, color=\"#fafafa\"),\n",
    "            ),\n",
    "            hole=0.75,\n",
    "            direction=\"clockwise\",\n",
    "            sort=False,\n",
    "            rotation=234,\n",
    "            showlegend=False,\n",
    "            hoverinfo=\"none\",\n",
    "            textinfo=\"none\",\n",
    "            textposition=\"outside\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if score is not None:\n",
    "        if display_score:\n",
    "            fig.add_trace(\n",
    "                go.Indicator(\n",
    "                    mode=\"number\", value=score, domain=dict(x=[0, 1], y=[0.28, 0.45])\n",
    "                )\n",
    "            )\n",
    "        fig.add_shape(\n",
    "            type=\"circle\", fillcolor=\"black\", x0=0.475, x1=0.525, y0=0.475, y1=0.525\n",
    "        )\n",
    "        fig.add_shape(\n",
    "            type=\"path\",\n",
    "            fillcolor=\"black\",\n",
    "            line=dict(width=0),\n",
    "            path=_generate_pointer_path(score),\n",
    "        )\n",
    "\n",
    "    return fig\n",
    "\n",
    "def create_gauge_chart(score: int) -> Image:\n",
    "    fig = gauge_and_needle_chart(score)\n",
    "    fig.update_layout(\n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        plot_bgcolor='rgba(0,0,0,0)',\n",
    "        margin=dict(t=20, b=20, l=20, r=20)\n",
    "    )\n",
    "    img_bytes = to_image(fig, format=\"png\", scale=2)\n",
    "    img = PILImage.open(io.BytesIO(img_bytes))\n",
    "    img_buffer = io.BytesIO()\n",
    "    img.save(img_buffer, format=\"PNG\")\n",
    "    img_buffer.seek(0)\n",
    "    return Image(img_buffer, width=1.75*inch, height=1.75*inch)\n",
    "\n",
    "def calculate_average_diversity_indexes(data: Dict[str, Any]) -> Tuple[float, float, List[str]]:\n",
    "    # Average diversity indexes, only for generated columns\n",
    "    # TODO: calibrate between text vs. categorical diversity scores before averaging them together\n",
    "    # TODO: move this calculation into the StandardEvaluationScores task \n",
    "    diversity_scores = []\n",
    "    gini_simpson_scores = []\n",
    "    text_diversity_scores = []\n",
    "    included_columns = []\n",
    "    column_notes = data['results']['column_notes']\n",
    "    try:\n",
    "        for key, value in data['results']['feature_distribution']['score'].items():\n",
    "            if isinstance(value, dict) and column_notes[key] == '':\n",
    "                included_columns.append(key)\n",
    "                if 'text_diversity_index' in value:\n",
    "                    diversity_scores.append(value['text_diversity_index'])\n",
    "                    text_diversity_scores.append(value['text_diversity_index'])\n",
    "                if 'gini_simpson_index' in value:\n",
    "                    diversity_scores.append(value['gini_simpson_index'])\n",
    "                    gini_simpson_scores.append(value['gini_simpson_index'])\n",
    "    except Exception as e:\n",
    "        print('Error calculating average diversity indexes:', e)\n",
    "\n",
    "    avg_diversity = sum(diversity_scores) / len(diversity_scores) if diversity_scores else None\n",
    "    avg_text_diversity = sum(text_diversity_scores) / len(text_diversity_scores) if text_diversity_scores else None\n",
    "    avg_gini_simpson = sum(gini_simpson_scores) / len(gini_simpson_scores) if gini_simpson_scores else None\n",
    "\n",
    "    return {'avg_diversity': avg_diversity, 'avg_text_diversity': avg_text_diversity, 'avg_gini_simpson': avg_gini_simpson}, included_columns\n",
    "\n",
    "def plot_distributions(data_results: Dict[str, Any], column_subset: str, story: List[Any], styles):\n",
    "    \"\"\"\n",
    "    Plot the distribution of each generated column or seed column.\n",
    "    data_results: expects data['results]\n",
    "    \"\"\"\n",
    "    assert column_subset in ['generated', 'seed'], f\"column_subset must be 'generated' or 'seed', not '{column_subset}'\"\n",
    "    column_note = \"\" if column_subset == 'generated' else \"Seed Column\"\n",
    "    plot_count = 0\n",
    "\n",
    "    try:\n",
    "        for key, distribution in data_results['feature_distribution']['distribution'].items():\n",
    "            # Calculate the total number of plots to determine if we need to add a page break\n",
    "            total_plot_count = sum(1 for v in data_results['column_notes'].values() if v == column_note)\n",
    "\n",
    "            # Only plot distributions for the subset of columns\n",
    "            if data_results['column_notes'][key] != column_note:\n",
    "                continue\n",
    "            \n",
    "            data_type = data_results['column_data_types'][key] if 'column_data_types' in data_results else None\n",
    "        \n",
    "            if distribution and isinstance(distribution, dict):\n",
    "                try:\n",
    "                    if 'score' in data_results['feature_distribution'] and key in data_results['feature_distribution']['score']:\n",
    "                        for score_key, score_value in data_results['feature_distribution']['score'][key].items():\n",
    "                            section_title = key.replace('_', ' ') + ' Distribution (' +score_key.replace('_', ' ')+ ': ' + str(round(score_value, 2)) + ')'\n",
    "                            section_title = section_title.replace('_', ' ').title()\n",
    "                            story.append(Paragraph(section_title, styles['Heading2']))\n",
    "                    \n",
    "                    if data_type == 'Categorical':\n",
    "                        dist_df = pd.DataFrame.from_dict(distribution, orient='index', columns=['count'])\n",
    "                        dist_df['count'] = pd.to_numeric(dist_df['count'], errors='coerce')\n",
    "                        dist_df = dist_df.dropna().sort_values('count', ascending=False)\n",
    "                        \n",
    "                        if not dist_df.empty:\n",
    "                            # Handle large distributions\n",
    "                            if len(dist_df) > 75:\n",
    "                                other_count = dist_df.iloc[75:]['count'].sum()\n",
    "                                dist_df = dist_df.iloc[:75]\n",
    "                                dist_df.loc['Other'] = other_count\n",
    "                            img = create_pareto_chart(dist_df, f\"{key.replace('_', ' ').title()} Distribution (Pareto Chart)\")\n",
    "                        else:\n",
    "                            continue\n",
    "                    \n",
    "                    elif data_type == 'Text':\n",
    "                        counts, bins = distribution['word_count_histogram']\n",
    "                        img = create_histogram(counts, bins, key, \"Text\")\n",
    "                        \n",
    "                    elif data_type == 'Numeric':\n",
    "                        counts = distribution['histogram']\n",
    "                        bins = distribution['bin_edges']\n",
    "                        img = create_histogram(counts, bins, key, \"Numeric\")\n",
    "                    else:\n",
    "                        # Skip unsupported column types, e.g., 'Other', None\n",
    "                        continue\n",
    "\n",
    "                    story.append(img)\n",
    "                    plot_count += 1\n",
    "\n",
    "                    # Fit 2 plots per page\n",
    "                    if plot_count % 2 == 0 and plot_count < total_plot_count:\n",
    "                        story.append(PageBreak())\n",
    "                    else:\n",
    "                        story.append(Spacer(1, 0.2*inch))\n",
    "                except Exception as e:\n",
    "                    story.append(Paragraph(f\"Error processing {key} distribution: {str(e)}\", styles['BodyText']))\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def create_report_pdf(data: Dict[str, Any], output_filename: str = 'enhanced_data_quality_report.pdf'):\n",
    "    doc = SimpleDocTemplate(output_filename, pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "    \n",
    "    chart_title_style = ParagraphStyle(\n",
    "        name='ChartTitle', \n",
    "        parent=styles['BodyText'], \n",
    "        alignment=TA_CENTER,\n",
    "        fontSize=8,\n",
    "        leading=10\n",
    "    )\n",
    "    styles.add(chart_title_style)\n",
    "\n",
    "    styles['Title'].fontSize = 24\n",
    "    styles['Title'].alignment = 1\n",
    "    styles['Title'].spaceAfter = 12\n",
    "    styles['Title'].textColor = colors.HexColor('#110420')\n",
    "\n",
    "    styles['Heading1'].fontSize = 18\n",
    "    styles['Heading1'].spaceAfter = 6\n",
    "    styles['Heading1'].textColor = colors.HexColor('#110420')\n",
    "\n",
    "    styles['Heading2'].fontSize = 14\n",
    "    styles['Heading2'].spaceBefore = 12\n",
    "    styles['Heading2'].spaceAfter = 6\n",
    "    styles['Heading2'].textColor = colors.HexColor('#110420')\n",
    "\n",
    "    styles['BodyText'].fontSize = 10\n",
    "    styles['BodyText'].spaceBefore = 6\n",
    "    styles['BodyText'].spaceAfter = 6\n",
    "    styles['BodyText'].textColor = colors.HexColor('#110420')\n",
    "\n",
    "    styles.add(ParagraphStyle(name='RowPreview',\n",
    "                              parent=styles['BodyText'],\n",
    "                              fontName='Courier',\n",
    "                              fontSize=8,\n",
    "                              leading=10,\n",
    "                              spaceAfter=12,\n",
    "                              firstLineIndent=0,\n",
    "                              leftIndent=20))\n",
    "    \n",
    "    story = []\n",
    "    \n",
    "    overview = data['dataset_overview_statistics']\n",
    "    data_results = data['results']\n",
    "    # Average diversity indexes, only for generated columns\n",
    "    avg_diversity_scores, included_columns = calculate_average_diversity_indexes(data)\n",
    "\n",
    "    story.append(Paragraph(\"Data Quality Report\", styles['Title']))\n",
    "    story.append(Spacer(1, 0.2*inch))\n",
    "\n",
    "    story.append(Paragraph(\"Key Metrics\", styles['Heading1']))\n",
    "    \n",
    "    # TODO: Add LLM as a judge score as a top level metric, potentially replacing % semantically unique \n",
    "    # Related, we may want to combine % semantically unique into the diversity score \n",
    "    unique_rows_chart = create_gauge_chart(int(data['results']['row_uniqueness']['percent_unique']))\n",
    "    semantically_unique_rows_chart = create_gauge_chart(int(data['results']['row_uniqueness']['percent_semantically_unique']))\n",
    "    diversity_chart = create_gauge_chart(int(avg_diversity_scores['avg_diversity'] * 100) if avg_diversity_scores['avg_diversity'] else None)\n",
    "    chart_list = [unique_rows_chart, semantically_unique_rows_chart, diversity_chart]\n",
    "\n",
    "    unique_rows_title = Paragraph(\"Unique Rows\", styles['ChartTitle'])\n",
    "    semantically_unique_rows_title = Paragraph(\"Semantically Unique Rows\", styles['ChartTitle'])\n",
    "    diversity_title = Paragraph(\"Data Diversity\", styles['ChartTitle'])\n",
    "    title_list = [unique_rows_title, semantically_unique_rows_title, diversity_title]\n",
    "\n",
    "    if 'valid_records_score' in data_results:\n",
    "        percent_valid_chart = create_gauge_chart(int(data_results['valid_records_score']['percent'] * 100))\n",
    "        percent_valid_title = Paragraph(\"Code Validity\", styles['ChartTitle'])\n",
    "        chart_list.append(percent_valid_chart)\n",
    "        title_list.append(percent_valid_title)\n",
    "    \n",
    "    if 'llm_as_a_judge_mean_scores' in data_results:\n",
    "        avg_llm_judge_score = sum(data_results['llm_as_a_judge_mean_scores'].values())/len(data_results['llm_as_a_judge_mean_scores'])\n",
    "        llm_judge_chart = create_gauge_chart(int((avg_llm_judge_score + 1) * 20))\n",
    "        llm_judge_title = Paragraph(\"Code Quality\", styles['ChartTitle'])\n",
    "        chart_list.append(llm_judge_chart)\n",
    "        title_list.append(llm_judge_title)\n",
    "    \n",
    "    if len(chart_list) > 4:\n",
    "        # Make room for quality scores haha, we can make this better later \n",
    "        chart_list.remove(semantically_unique_rows_chart)\n",
    "        title_list.remove(semantically_unique_rows_title)\n",
    "\n",
    "    chart_table = Table([title_list, chart_list])\n",
    "    chart_table_style = TableStyle([\n",
    "        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
    "        ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),\n",
    "        ('BOTTOMPADDING', (0, 0), (-1, 0), 2), \n",
    "        ('TOPPADDING', (0, 1), (-1, -1), 3),\n",
    "    ])\n",
    "    chart_table.setStyle(chart_table_style)\n",
    "\n",
    "    story.append(chart_table)\n",
    "\n",
    "    # Dataset Overview\n",
    "    story.append(Paragraph(\"Dataset Overview\", styles['Heading1']))\n",
    "    story.append(Paragraph(\"This section provides key metrics on the structure, uniqueness, complexity, and quality of the data.\", styles['BodyText']))\n",
    "\n",
    "    # Split the overview data into two tables to save space\n",
    "    overview_data_1 = [\n",
    "        [\"Metric\", \"Value\"],\n",
    "        [\"Data Completeness\", f\"{overview['data_completeness']}%\"],\n",
    "        [\"Number of Rows\", f\"{overview['number_of_rows']}\"],\n",
    "        [\"Number of Columns\", f\"{overview['number_of_columns']}\"],\n",
    "        [\"Categorical Columns\", f\"{overview['number_of_categorical_columns']}\"],\n",
    "        [\"Text Columns\", f\"{overview['number_of_text_columns']}\"],\n",
    "        [\"Numerical Columns\", f\"{overview['number_of_numerical_columns']}\"],\n",
    "        [\"Seed Columns\", f\"{overview['number_of_seed_columns']}\"], \n",
    "    ]\n",
    "\n",
    "    overview_data_2 = [\n",
    "        [\"Metric\", \"Value\"],\n",
    "        [\"Unique Rows\", f\"{data['results']['row_uniqueness']['percent_unique']}%\"],\n",
    "        [\"Semantically Unique Rows\", f\"{data_results['row_uniqueness']['percent_semantically_unique']}%\"],\n",
    "        [\"Avg Words per Row\", f\"{data_results['num_words_per_record']['average_words_per_record']:.2f}\"],\n",
    "        [\"Avg Tokens per Row\", f\"{data_results['num_words_per_record']['average_tokens_per_record']:.2f}\"],\n",
    "        [\"Total Tokens\", f\"{data_results['num_words_per_record']['total_tokens']}\"],\n",
    "        [\"Avg Text Diversity\", f\"{avg_diversity_scores['avg_text_diversity']:.4f}\" if avg_diversity_scores['avg_text_diversity'] else \"N/A\"],\n",
    "        [\"Avg Gini-Simpson Index\", f\"{avg_diversity_scores['avg_gini_simpson']:.4f}\" if avg_diversity_scores['avg_gini_simpson'] else \"N/A\"],\n",
    "    ]\n",
    "\n",
    "    overview_table_1 = create_overview_table(overview_data_1)\n",
    "    overview_table_2 = create_overview_table(overview_data_2)\n",
    "    overview_table_table = Table([[overview_table_1, overview_table_2]])\n",
    "    story.append(overview_table_table)\n",
    "    story.append(Spacer(1, 0.2*inch))\n",
    "\n",
    "    # Single Row Preview\n",
    "    # TODO: cut off the preview if it would spill into the next page?\n",
    "    story.append(Paragraph(\"Single Row Preview\", styles['Heading1']))\n",
    "    preview_text = create_single_record_preview(data['dataset_overview_statistics']['single_row'])\n",
    "    story.append(Paragraph(preview_text, styles['RowPreview']))\n",
    "    story.append(PageBreak())\n",
    "\n",
    "    # Dataset Schema\n",
    "    story.append(Paragraph(\"Dataset Schema & Preview\", styles['Heading1']))\n",
    "    story.append(Paragraph(\"The schema table provides an overview of each column in the dataset.\", styles['BodyText']))\n",
    "    schema_table = create_schema_table(data)\n",
    "    story.append(schema_table)\n",
    "    story.append(Spacer(1, 0.2*inch))\n",
    "\n",
    "    # column Cardinality\n",
    "    if 'feature_cardinality' in data_results:\n",
    "        story.append(Paragraph(\"Column Cardinality\", styles['Heading1']))\n",
    "        feature_cardinality = pd.DataFrame.from_dict(data_results['feature_cardinality'], orient='index', columns=['cardinality'])\n",
    "        \n",
    "        img = create_chart(feature_cardinality['cardinality'], \"Column Cardinality\", \"Columns\", \"Cardinality\")\n",
    "        story.append(img)\n",
    "        story.append(PageBreak())\n",
    "\n",
    "    # Distribution Visualizations\n",
    "    story.append(Paragraph(\"Seed Column Distributions\", styles['Heading1']))\n",
    "    plot_distributions(data_results, 'seed', story, styles)\n",
    "    story.append(PageBreak())\n",
    "    story.append(Paragraph(\"Generated Column Distributions\", styles['Heading1']))\n",
    "    plot_distributions(data_results, 'generated', story, styles)\n",
    "    story.append(PageBreak())\n",
    "    \n",
    "    \n",
    "    # Word Count per Column\n",
    "    if 'word_counts_per_column' in data_results['num_words_per_record']:\n",
    "        story.append(Paragraph(\"Average Word Count per Column\", styles['Heading1']))\n",
    "        word_count = pd.DataFrame.from_dict(data_results['num_words_per_record']['word_counts_per_column'], orient='index', columns=['avg_words'])\n",
    "        word_count = word_count.sort_values('avg_words', ascending=False)\n",
    "        \n",
    "        img = create_chart(word_count['avg_words'], \"Average Word Count per Column\", \"Columns\", \"Average Word Count\")\n",
    "        story.append(img)\n",
    "        story.append(Spacer(1, 0.2*inch))\n",
    "\n",
    "    # Text Diversity Indices\n",
    "    if 'feature_distribution' in data_results and 'score' in data_results['feature_distribution']:\n",
    "        text_diversity = {}\n",
    "        for key, value in data_results['feature_distribution']['score'].items():\n",
    "            if isinstance(value, dict) and 'text_diversity_index' in value:\n",
    "                text_diversity[key] = value['text_diversity_index']\n",
    "        if text_diversity:\n",
    "            story.append(Paragraph(\"Text Diversity Indices\", styles['Heading1']))\n",
    "            text_diversity_df = pd.DataFrame.from_dict(text_diversity, orient='index', columns=['diversity_index'])\n",
    "            img = create_text_diversity_chart(text_diversity_df)\n",
    "            story.append(img)\n",
    "            story.append(Spacer(1, 0.2*inch))\n",
    "            \n",
    "\n",
    "    # Conclusion\n",
    "    story.append(Paragraph(\"Conclusion\", styles['Heading1']))\n",
    "\n",
    "    conclusion_text = \"This report provides a comprehensive view of the dataset's structure, content diversity, and the nature of the data it contains. Key takeaways include:<br/>\"\n",
    "\n",
    "    # Data Uniqueness\n",
    "    if 'row_uniqueness' in data_results:\n",
    "        unique = data_results['row_uniqueness'].get('percent_unique', 'N/A')\n",
    "        sem_unique = data_results['row_uniqueness'].get('percent_semantically_unique', 'N/A')\n",
    "        conclusion_text += f\"1. Data Uniqueness: With {unique}% unique rows and {sem_unique}% semantically unique rows, \"\n",
    "        if unique != 'N/A' and float(unique) > 90:\n",
    "            conclusion_text += \"the dataset shows a high degree of individuality in its rows. This suggests a rich and varied dataset.<br/><br/>\"\n",
    "        else:\n",
    "            conclusion_text += \"the dataset shows some level of repetition in its rows. This may indicate patterns or recurring themes in the data.<br/><br/>\"\n",
    "\n",
    "    # column Cardinality\n",
    "    if 'feature_cardinality' in data_results:\n",
    "        conclusion_text += \"2. Column Cardinality: The dataset contains columns with varying cardinalities. \"\n",
    "        conclusion_text += \"This diversity in column types allows for both granular analysis and higher-level pattern recognition.<br/><br/>\"\n",
    "\n",
    "    # Distribution Patterns\n",
    "    if 'feature_distribution' in data_results and 'distribution' in data_results['feature_distribution']:\n",
    "        conclusion_text += \"3. Distribution Patterns: The charts reveal the distribution patterns within each column, \"\n",
    "        conclusion_text += \"highlighting potential focus areas or biases in the data. Understanding these distributions \"\n",
    "        conclusion_text += \"is crucial for balanced analysis and identifying underrepresented categories.<br/><br/>\"\n",
    "\n",
    "    # Text Complexity\n",
    "    if 'num_words_per_record' in data_results:\n",
    "        avg_words = data_results['num_words_per_record'].get('average_words_per_record', 'N/A')\n",
    "        if avg_words != 'N/A':\n",
    "            conclusion_text += f\"4. Text Complexity: With an average of {avg_words:.2f} words per row, \"\n",
    "            if float(avg_words) > 50:\n",
    "                conclusion_text += \"the dataset shows a high level of complexity. \"\n",
    "            elif float(avg_words) > 20:\n",
    "                conclusion_text += \"the dataset shows a moderate level of complexity. \"\n",
    "            else:\n",
    "                conclusion_text += \"the dataset shows a low level of complexity. \"\n",
    "            conclusion_text += \"This gives an indication of the depth of information contained in each row.<br/><br/>\"\n",
    "\n",
    "    # Text Diversity\n",
    "    if 'feature_distribution' in data_results and 'score' in data_results['feature_distribution']:\n",
    "        conclusion_text += \"5. Text Diversity: The text diversity indices provide insight into the variety of content within text columns. \"\n",
    "        conclusion_text += \"Higher diversity can be beneficial for tasks requiring a broad range of examples, while lower diversity \"\n",
    "        conclusion_text += \"might indicate more standardized content.<br/><br/>\"\n",
    "\n",
    "    conclusion_text += \"\"\"\n",
    "    <b>Implications for Machine Learning:</b><br/><br/> \n",
    " \n",
    "    <b>Pre-training</b><br/> \n",
    "    - The dataset's uniqueness and diversity can provide a rich foundation for pre-training language models or other AI systems.<br/>\n",
    "    - High cardinality columns may help in learning broad representations, while low cardinality columns could aid in learning important categorical distinctions.<br/>\n",
    "    - If text diversity is high, it could be particularly valuable for building robust language models that can handle a wide range of contexts and styles.<br/><br/>\n",
    "\n",
    "    <b>Fine-tuning:</b><br/> \n",
    "    - The distribution patterns revealed in the charts should guide the fine-tuning process. Imbalanced categories may require techniques like weighted sampling or loss adjustment to ensure equal representation during fine-tuning.<br/>\n",
    "    - Columns with high semantic uniqueness could be especially useful for fine-tuning models on specific domains or tasks, as they likely contain a wide range of relevant examples.<br/>\n",
    "    - Consider the average word count per row when deciding on sequence length for transformer-based models during fine-tuning.<br/><br/>\n",
    "\n",
    "    <b>Designing/Iterating on Data to Fill Data Gaps:</b><br/> \n",
    "    - Analyze the distribution charts to identify underrepresented categories. These areas may require additional data collection or augmentation to ensure comprehensive model performance.<br/>\n",
    "    - If certain text diversity scores are low, consider ways to introduce more variety in those columns, either through data augmentation techniques or targeted data collection.<br/>\n",
    "    - For columns with very high cardinality, consider if grouping or categorization might be beneficial to prevent overfitting on rare categories.<br/>\n",
    "    - If semantic uniqueness is low in certain areas, it might indicate a need for more diverse examples in those categories to improve model generalization.<br/><br/>\n",
    "\n",
    "    <b>General Considerations:</b><br/> \n",
    "    - The overall uniqueness of the dataset impacts models that require diverse examples. However, care should be taken to address any imbalances revealed in the distribution charts.<br/>\n",
    "    - Monitor for potential biases in the data that could be propagated or amplified by machine learning models.<br/>\n",
    "    - Consider privacy implications, especially for high-cardinality columns that might contain identifiable information.<br/>\n",
    "    - The text complexity (average words per row) should inform decisions about model architecture and preprocessing steps.<br/><br/>\n",
    "    \"\"\"\n",
    "\n",
    "    story.append(Paragraph(conclusion_text, styles['BodyText']))\n",
    "\n",
    "    metric_definition_text = f\"\"\"\n",
    "    This section provides definitions for the metrics used in the report.<br/><br/>\n",
    "    <b>Key Metrics</b><br/>\n",
    "    Only generated columns requested by the user are included in the calculation of Key Metrics: {included_columns}. Helper columns like ID columns, seed columns or informational columns like code validation columns, data quality evaluation columns are excluded from Key Metrics calculation. <br/>\n",
    "    ‚Ä¢ <b>Unique Rows:</b> Percentage of rows that are unique in the dataset.<br/>\n",
    "    ‚Ä¢ <b>Semantically Unique Rows:</b> Percentage of rows that are semantically unique, based on TF-IDF.<br/>\n",
    "    ‚Ä¢ <b>Text Diversity:</b> Average Text Diversity Index (defined below) across all text columns, with higher values indicating more diverse content.<br/>\n",
    "    ‚Ä¢ <b>Gini-Simpson Diversity:</b> Average Gini-Simpson Index (defined below) across all categorical columns. Higher values indicating greater diversity.<br/><br/>\n",
    "\n",
    "    <b>Dataset Overview</b><br/>\n",
    "    The enhanced dataset overview provides key metrics about the structure, uniqueness, complexity, and quality of the data:<br/>\n",
    "    ‚Ä¢ <b>Number of Rows and Columns:</b> Indicates the size and dimensionality of the dataset.<br/>\n",
    "    ‚Ä¢ <b>Categorical and Numerical Columns:</b> Gives insight into the types of data present, helping to guide appropriate analysis techniques.<br/>\n",
    "    ‚Ä¢ <b>Data Completeness:</b> Shows the overall percentage of non-null values across all columns, indicating the dataset's overall quality and potential need for imputation.<br/>\n",
    "    ‚Ä¢ <b>Unique and Semantically Unique Rows:</b> Demonstrates the level of data diversity and potential redundancy in the dataset.<br/>\n",
    "    ‚Ä¢ <b>Average Words per Row:</b> Provides an indication of the typical complexity or detail level of each entry.<br/>\n",
    "    ‚Ä¢ <b>Average Tokens per Row and Total Tokens:</b> These metrics correspond to tokens used in Large Language Models (LLMs), giving an estimate of the dataset's complexity from an LLM processing perspective.<br/>\n",
    "    ‚Ä¢ <b>Average Text Diversity:</b> Average Text Diversity Index (defined below) across all text columns, with higher values indicating more diverse content.<br/>\n",
    "    ‚Ä¢ <b>Average Gini-Simpson Index:</b> Average Gini-Simpson Index (defined below) across all categorical columns. Higher values indicating greater diversity.<br/><br/>\n",
    "\n",
    "    <b>Dataset Schema & Preview</b><br/>\n",
    "    The schema table provides an overview of each column in the dataset, including the data type, the count of non-null and null values, and the average length (where applicable). This information is crucial for understanding the structure of the data and identifying potential data quality issues such as missing values or unexpected data types.<br/>\n",
    "    ‚Ä¢ <b>Data Type:</b> Categorical, Numeric, Text or Other. Categorical columns are those whose percentage of unique values are low; Text columns are non-Categorical columns with at least 2 spaces per Row, on average.<br/>\n",
    "    ‚Ä¢ <b>Total Count:</b> Total number of values in the Column.<br/>\n",
    "    ‚Ä¢ <b>% Null:</b> Percentage of null values in the Column.<br/>\n",
    "    ‚Ä¢ <b>Average Length:</b> Average character count of the values (for each text column).<br/>\n",
    "    ‚Ä¢ <b>Avg Tokens:</b> Average number of tokens in the values (for each text column).<br/><br/>\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if 'feature_cardinality' in data_results:\n",
    "        metric_definition_text += \"\"\"\n",
    "        <b>Column Cardinality</b><br/>\n",
    "        ‚Ä¢ <b>Column cardinality:</b> Represents the number of unique values for each column in the dataset. Higher cardinality indicates more diverse values within a Column.<br/><br/>\n",
    "\n",
    "        \"\"\"\n",
    "    \n",
    "    metric_definition_text += \"\"\"\n",
    "    <b>Column Distributions</b><br/>\n",
    "    Column distributions show the frequency of different values within each column. These visualizations help identify common patterns, imbalances, or biases in the data.<br/>\n",
    "    ‚Ä¢ <b>Pareto Chart:</b> A Pareto chart illustrates the distribution of domain in the dataset. The bars represent the count for each category, while the line shows the cumulative percentage. Only the top 75 categories are shown individually. The remaining categories are grouped as 'Other'. This visualization helps identify the most significant categories and their relative importance.<br/>\n",
    "    ‚Ä¢ <b>Gini-Simpson Index:</b> A diversity index for categorical columns. It quantifies the probability that two values taken at random from the column (with replacement) are different. Higher values indicate greater diversity.<br/>\n",
    "    ‚Ä¢ <b>Text Diversity Index:</b> A diversity index for text columns. It is defined as the average correlation between each row's TF-IDF vector and the dataset's TF-IDF matrix. Higher values indicate greater diversity.<br/><br/>\n",
    "    \"\"\"\n",
    "\n",
    "    story.append(Paragraph(\"Metric Definitions\", styles['Heading1']))\n",
    "    story.append(Paragraph(metric_definition_text, styles['BodyText']))\n",
    "\n",
    "    # Build the PDF\n",
    "    doc.build(story)\n",
    "    print(f\"PDF created: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report_pdf(results, dataset_df,  'data_quality_report.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# field = 'Example'\n",
    "# counts = [7, 7, 7, 10, 6, 6, 3, 1, 1, 2]\n",
    "# bins = [0.0, 27.7, 55.4, 83.1, 110.8, 138.5, 166.2, 193.9, 221.6, 249.29999999999998, 277.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"here's the {prompt} and here's the {response}\"\n",
    "test_str.format(prompt='prompt', response='response', context='context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['results']['llm_as_a_judge_mean_scores']\n",
    "results['results']['valid_records_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['dataset_overview_statistics']['dtypes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "navhelpers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
