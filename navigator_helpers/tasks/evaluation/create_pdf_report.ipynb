{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a PDF of the Data Quality Report for Data Desginer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéõÔ∏è Import things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from navigator_helpers.llms.llm_suite import GretelLLMSuite\n",
    "from navigator_helpers.tasks.evaluation.evaluation import (\n",
    "    BaseEvaluationTaskSuite,\n",
    "    VisualizationTaskSuite\n",
    ")\n",
    "\n",
    "# set environment variable 'GRETEL_PROD_API_KEY' from https://console.gretel.ai/users/me/key\n",
    "gretel_prod_api_key = input(\"Enter your Gretel API key from https://console.gretel.ai/users/me/key: \")\n",
    "os.environ['GRETEL_PROD_API_KEY'] = gretel_prod_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¢ Choose Dataset for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of samples to load from the dataset for testing\n",
    "# Set to None to use the full dataset\n",
    "NUM_SAMPLES = 1000\n",
    "\n",
    "datasets_dict = {\n",
    "    \"synthetic_text_to_sql\": {\n",
    "        \"dataset_kwargs\": {\n",
    "            \"path\": \"gretelai/synthetic_text_to_sql\",\n",
    "            \"split\": \"train\"\n",
    "        },\n",
    "        \"code_lang\": \"sql\",\n",
    "        \"eval_kwargs\":{\n",
    "            \"instruction_col_name\": \"sql_prompt\",\n",
    "            \"code_col_name\": \"sql\",\n",
    "            \"context_col_name\": \"sql_context\"\n",
    "        }\n",
    "    },\n",
    "    \"gsm8k\": {\n",
    "        \"dataset_kwargs\": {\n",
    "            \"path\": \"openai/gsm8k\",\n",
    "            \"name\": \"main\",\n",
    "            \"split\": \"train\"\n",
    "        },\n",
    "        \"eval_kwargs\": {\n",
    "            \"instruction_col_name\": \"question\",\n",
    "            \"code_col_name\": \"answer\",\n",
    "        }\n",
    "    },\n",
    "    \"synthetic_gsm8k\": {\n",
    "        \"dataset_kwargs\": {\n",
    "            \"path\": \"gretelai/synthetic-gsm8k-reflection-405b\",\n",
    "            \"split\": \"train\"\n",
    "        },\n",
    "        \"eval_kwargs\": {\n",
    "            \"instruction_col_name\": \"question\",\n",
    "            \"code_col_name\": \"answer\",\n",
    "        }\n",
    "    },\n",
    "    \"xlcost_text_to_code\": {\n",
    "        \"dataset_kwargs\": {\n",
    "            \"path\": \"codeparrot/xlcost-text-to-code\",\n",
    "            \"split\": \"train\"\n",
    "        },\n",
    "        \"code_lang\": \"python\",\n",
    "        \"eval_kwargs\": {\n",
    "            \"instruction_col_name\": \"text\",\n",
    "            \"code_col_name\": \"code\",\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# Prompt user to select a dataset\n",
    "print(\"Available datasets:\")\n",
    "for key in datasets_dict.keys():\n",
    "    print(f\" - {key}\")\n",
    "\n",
    "selected_dataset = input(\"\\nEnter the name of the dataset to load: \").strip()\n",
    "\n",
    "# Load the selected dataset\n",
    "if selected_dataset in datasets_dict:\n",
    "    dataset_dict = datasets_dict[selected_dataset]\n",
    "    eval_kwargs = dataset_dict[\"eval_kwargs\"]\n",
    "    code_lang = dataset_dict[\"code_lang\"] if \"code_lang\" in dataset_dict.keys() else None\n",
    "    dataset = load_dataset(**dataset_dict[\"dataset_kwargs\"])\n",
    "\n",
    "    # Optionally, select a subset if NUM_SAMPLES is specified\n",
    "    if NUM_SAMPLES is not None and NUM_SAMPLES < len(dataset):\n",
    "        dataset = dataset.select(range(NUM_SAMPLES))\n",
    "    \n",
    "    dataset_df = dataset.to_pandas()\n",
    "    \n",
    "    print(f\"Loaded dataset '{selected_dataset}' successfully!\")\n",
    "else:\n",
    "    print(\"Error: Dataset not found. Please enter a valid dataset name.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_suite = GretelLLMSuite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to store evaluation results\n",
    "results = {}\n",
    "\n",
    "# Uncomment the following lines to run individual evaluation tasks\n",
    "results.update({\"row_uniqueness\": BaseEvaluationTaskSuite(llm_suite, dataset_df).row_uniqueness()})\n",
    "results.update({\"feature_cardinality\": BaseEvaluationTaskSuite(llm_suite, dataset_df).feature_cardinality()})\n",
    "results.update({\"feature_distribution\": BaseEvaluationTaskSuite(llm_suite, dataset_df).feature_distribution()})\n",
    "results.update({\"num_words_per_record\": BaseEvaluationTaskSuite(llm_suite, dataset_df).num_words_per_record()})\n",
    "\n",
    "# Uncomment this line to run everything, including LLM-as-a-judge\n",
    "# results = BaseEvaluationTaskSuite(llm_suite, dataset_df, code_lang, eval_kwargs).evaluate_all()\n",
    "\n",
    "pprint(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from typing import List, Optional, Tuple, Union, Dict, Any\n",
    "import math\n",
    "import io\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.io import to_image\n",
    "from PIL import Image as PILImage\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.platypus import SimpleDocTemplate, PageBreak, Paragraph, Spacer, Image, Table, TableStyle\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.lib.enums import TA_CENTER\n",
    "\n",
    "# Constants\n",
    "FIG_WIDTH = 7.5  # inches\n",
    "FIG_HEIGHT = 5  # inches\n",
    "SCORE_VALUES = [\n",
    "    {\"label\": \"Very poor\", \"color\": \"rgb(229, 60, 26)\"},\n",
    "    {\"label\": \"Poor\", \"color\": \"rgb(229, 128, 26)\"},\n",
    "    {\"label\": \"Average\", \"color\": \"rgb(229, 161, 26)\"},\n",
    "    {\"label\": \"Good\", \"color\": \"rgb(183, 210, 45)\"},\n",
    "    {\"label\": \"Excellent\", \"color\": \"rgb(72, 210, 45)\"},\n",
    "]\n",
    "PRIMARY_PALETTE = ['#2E1065', '#D3A66E', '#110420', '#4F00A9', '#F9EFDE', '#1D0B32', '#8D32FA', '#C399FF', '#EFE5FF', '#EFD7AD', '#F4E3C6', '#FBF7ED', '#A59DAD', '#D2CED6', '#E8E7EB']\n",
    "SECONDARY_PALETTE = ['#052095', '#FF6BA9', '#3056F2', '#FFA8CC', '#8BB9FF', '#FFEDF5', '#E5F0FF', '#1E9C98', '#92F6F4', '#C5FEFF', '#E8FEFF', '#FF9248', '#FFB38A', '#FFD7B5', '#FFECDC', '#FF6700', '#FFCA1A', '#FFE16D', '#FFF099', '#FFFDE3', '#ECA10A']\n",
    "\n",
    "# Set up custom color palette for seaborn\n",
    "sns.set_theme(style=\"white\")\n",
    "sns.set_palette(sns.color_palette(SECONDARY_PALETTE))\n",
    "\n",
    "def create_chart(data: pd.Series, title: str, xlabel: str, ylabel: str) -> Image:\n",
    "    fig, ax = plt.subplots(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "    bars = ax.bar(range(len(data)), data.values, color='#4F00A9')\n",
    "    ax.set_facecolor('white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    ax.set_title(title, fontsize=10, color='#1D0B32')\n",
    "    ax.set_xlabel(xlabel, fontsize=10, color='#1D0B32')\n",
    "    ax.set_ylabel(ylabel, fontsize=10, color='#1D0B32')\n",
    "    ax.set_xticks(range(len(data)))\n",
    "    \n",
    "    truncated_labels = [str(label)[:25] + ('...' if len(str(label)) > 25 else '') for label in data.index]\n",
    "    ax.set_xticklabels(truncated_labels, rotation=45, ha='right', fontsize=8, color='#1D0B32')\n",
    "    \n",
    "    ax.tick_params(axis='both', colors='#1D0B32')\n",
    "    ax.tick_params(axis='y', labelsize=6)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}' if isinstance(height, float) else f'{height}',\n",
    "                ha='center', va='bottom', fontsize=8, color='#1D0B32')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    img_buffer = io.BytesIO()\n",
    "    plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')\n",
    "    img_buffer.seek(0)\n",
    "    return Image(img_buffer, width=7*inch, height=4.5*inch)\n",
    "\n",
    "def create_pareto_chart(data: pd.DataFrame, title: str) -> Image:\n",
    "    fig, ax1 = plt.subplots(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    bars = ax1.bar(range(len(data)), data['count'], color='#4F00A9')\n",
    "    ax1.set_facecolor('white')\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    ax1.set_xlabel('Categories', fontsize=10, color='#1D0B32')\n",
    "    ax1.set_ylabel('Count', fontsize=10, color='#1D0B32')\n",
    "    ax1.set_title(title, fontsize=10, color='#1D0B32')\n",
    "    \n",
    "    cumulative_percentage = 100 * data['count'].cumsum() / data['count'].sum()\n",
    "    ax2.plot(range(len(data)), cumulative_percentage, color='#FF6700', marker='D', ms=4)\n",
    "    ax2.set_ylabel('Cumulative Percentage', fontsize=10, color='#1D0B32')\n",
    "    ax2.set_ylim([0, 110])\n",
    "    \n",
    "    ax1.tick_params(axis='both', colors='#1D0B32')\n",
    "    ax2.tick_params(axis='both', colors='#1D0B32')\n",
    "    ax1.tick_params(axis='y', labelsize=6)\n",
    "    ax2.tick_params(axis='y', labelsize=6)\n",
    "    \n",
    "    ax1.set_xticks(range(len(data)))\n",
    "    truncated_labels = [str(label)[:25] + ('...' if len(str(label)) > 25 else '') for label in data.index]\n",
    "    ax1.set_xticklabels(truncated_labels, rotation=45, ha='right', fontsize=6, color='#1D0B32')\n",
    "    \n",
    "    for i, v in enumerate(data['count']):\n",
    "        ax1.text(i, v, f'{v:.2f}' if isinstance(v, float) else f'{v}', ha='center', va='bottom', fontsize=8, color='#1D0B32')\n",
    "    \n",
    "    ax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.0f}%'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    img_buffer = io.BytesIO()\n",
    "    plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')\n",
    "    img_buffer.seek(0)\n",
    "    return Image(img_buffer, width=7*inch, height=4.5*inch)\n",
    "\n",
    "def create_text_diversity_chart(text_diversity_df: pd.DataFrame) -> Image:\n",
    "    plt.figure(figsize=(FIG_WIDTH, FIG_HEIGHT))\n",
    "    ax = sns.barplot(x=text_diversity_df.index, y='diversity_index', data=text_diversity_df, color='#4F00A9')\n",
    "    ax.set_facecolor('white')\n",
    "    plt.gcf().patch.set_facecolor('white')\n",
    "    \n",
    "    plt.title(\"Text Diversity Indices\", fontsize=10, color='#1D0B32')\n",
    "    plt.ylabel(\"Diversity Index\", fontsize=10, color='#1D0B32')\n",
    "    plt.xlabel(\"\", fontsize=10, color='#1D0B32')\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=8, color='#1D0B32')\n",
    "    \n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    ax.tick_params(axis='both', colors='#1D0B32')\n",
    "    ax.tick_params(axis='y', labelsize=6)\n",
    "    \n",
    "    for i, v in enumerate(text_diversity_df['diversity_index']):\n",
    "        ax.text(i, v, f'{v:.2f}', ha='center', va='bottom', fontsize=8, color='#1D0B32')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    img_buffer = io.BytesIO()\n",
    "    plt.savefig(img_buffer, format='png', dpi=300, bbox_inches='tight')\n",
    "    img_buffer.seek(0)\n",
    "    return Image(img_buffer, width=7*inch, height=4.5*inch)\n",
    "\n",
    "def create_schema_table(dataset_df: pd.DataFrame, data: Dict[str, Any]) -> Tuple[Table, Dict[str, float]]:\n",
    "    schema_data = [['Column Name', 'Data Type', 'Total Count', '% Null', 'Average Length', 'Avg Tokens']]\n",
    "    avg_lengths = {}\n",
    "    for col in dataset_df.columns:\n",
    "        dtype = str(dataset_df[col].dtype)\n",
    "        total_count = len(dataset_df)\n",
    "        null_count = dataset_df[col].isnull().sum()\n",
    "        pcnt_null = (null_count / total_count) * 100\n",
    "        avg_length = 'N/A'\n",
    "        avg_tokens = 'N/A'\n",
    "        if 'feature_distribution' in data and 'distribution' in data['feature_distribution']:\n",
    "            if col in data['feature_distribution']['distribution']:\n",
    "                col_data = data['feature_distribution']['distribution'][col]\n",
    "                if isinstance(col_data, dict):\n",
    "                    if 'avg_length' in col_data:\n",
    "                        avg_length = f\"{col_data['avg_length']:.2f}\"\n",
    "                        avg_lengths[col] = col_data['avg_length']\n",
    "                    if 'avg_tokens' in col_data:\n",
    "                        avg_tokens = f\"{col_data['avg_tokens']:.2f}\"\n",
    "        schema_data.append([col, dtype, total_count, f\"{pcnt_null:.2f}%\", avg_length, avg_tokens])\n",
    "    \n",
    "    table = Table(schema_data)\n",
    "    style = TableStyle([\n",
    "        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#4F00A9')),\n",
    "        ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),\n",
    "        ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
    "        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "        ('FONTSIZE', (0, 0), (-1, 0), 8),\n",
    "        ('BOTTOMPADDING', (0, 0), (-1, 0), 6),\n",
    "        ('BACKGROUND', (0, 1), (-1, -1), colors.HexColor('#EFE5FF')),\n",
    "        ('TEXTCOLOR', (0, 1), (-1, -1), colors.HexColor('#110420')),\n",
    "        ('ALIGN', (0, 1), (-1, -1), 'LEFT'),\n",
    "        ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),\n",
    "        ('FONTSIZE', (0, 1), (-1, -1), 7),\n",
    "        ('TOPPADDING', (0, 1), (-1, -1), 3),\n",
    "        ('BOTTOMPADDING', (0, 1), (-1, -1), 3),\n",
    "        ('GRID', (0, 0), (-1, -1), 1, colors.HexColor('#4F00A9'))\n",
    "    ])\n",
    "    table.setStyle(style)\n",
    "    return table, avg_lengths\n",
    "\n",
    "def create_overview_table(overview_data: List[List[str]]) -> Table:\n",
    "    table = Table(overview_data, colWidths=[3*inch, 3*inch])\n",
    "    style = TableStyle([\n",
    "        ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#4F00A9')),\n",
    "        ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),\n",
    "        ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
    "        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "        ('FONTSIZE', (0, 0), (-1, 0), 8),\n",
    "        ('BOTTOMPADDING', (0, 0), (-1, 0), 3),  # Reduced padding\n",
    "        ('BACKGROUND', (0, 1), (-1, -1), colors.HexColor('#EFE5FF')),\n",
    "        ('TEXTCOLOR', (0, 1), (-1, -1), colors.HexColor('#110420')),\n",
    "        ('ALIGN', (0, 1), (-1, -1), 'LEFT'),\n",
    "        ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),\n",
    "        ('FONTSIZE', (0, 1), (-1, -1), 7),\n",
    "        ('TOPPADDING', (0, 1), (-1, -1), 3),  # Minimal top padding\n",
    "        ('BOTTOMPADDING', (0, 1), (-1, -1), 3),  # Minimal bottom padding\n",
    "        ('GRID', (0, 0), (-1, -1), 0.5, colors.HexColor('#4F00A9'))  # Thinner grid lines\n",
    "    ])\n",
    "    table.setStyle(style)\n",
    "    return table\n",
    "\n",
    "def create_single_record_preview(dataset_df: pd.DataFrame) -> str:\n",
    "    record = dataset_df.iloc[0]\n",
    "    preview_text = \"\"\n",
    "    for column, value in record.items():\n",
    "        truncated_value = str(value)[:100] + ('...' if len(str(value)) > 100 else '')\n",
    "        preview_text += f\"<b>{column}:</b>\\t{truncated_value}\"\n",
    "        preview_text += \"<br/>\"\n",
    "    return preview_text\n",
    "\n",
    "def _generate_pointer_path(score: int) -> str:\n",
    "    theta = score * (282 - 34) / 100 - 34\n",
    "    rads = math.radians(theta)\n",
    "    radius = 0.45\n",
    "    size = 0.025\n",
    "    x1 = -1 * radius * math.cos(rads) + 0.5\n",
    "    y1 = radius * math.sin(rads) + 0.5\n",
    "    return f\"\"\"\n",
    "    M {x1} {y1}\n",
    "    L {-1 * size * math.cos(math.radians(theta - 90)) + 0.5}\n",
    "        {size * math.sin(math.radians(theta - 90)) + 0.5}\n",
    "    L {-1 * size * math.cos(math.radians(theta + 90)) + 0.5}\n",
    "        {size * math.sin(math.radians(theta + 90)) + 0.5}\n",
    "    Z\"\"\"\n",
    "\n",
    "def gauge_and_needle_chart(score: Optional[int], display_score: bool = True, marker_colors: Optional[List[str]] = None) -> go.Figure:\n",
    "    if score is None:\n",
    "        fig = go.Figure(\n",
    "            layout=go.Layout(\n",
    "                annotations=[\n",
    "                    go.layout.Annotation(\n",
    "                        text=\"N/A\",\n",
    "                        font=dict(color=\"rgba(174, 95, 5, 1)\", size=18),\n",
    "                        showarrow=False,\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\",\n",
    "                        x=0.5,\n",
    "                        y=0.5,\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        marker_colors = [\"rgb(220, 220, 220)\", \"rgba(255, 255, 255, 0)\"]\n",
    "        pie_values = [70, 30]\n",
    "    else:\n",
    "        if not marker_colors:\n",
    "            marker_colors = [s[\"color\"] for s in SCORE_VALUES]\n",
    "        if marker_colors[-1] != \"rgba(255, 255, 255, 0)\":\n",
    "            marker_colors.append(\"rgba(255, 255, 255, 0)\")\n",
    "        pie_values = [70 // (len(marker_colors) - 1)] * (len(marker_colors) - 1)\n",
    "        pie_values.append(30)\n",
    "        fig = go.Figure()\n",
    "\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        showlegend=False,\n",
    "        xaxis=dict(visible=False),\n",
    "        yaxis=dict(visible=False),\n",
    "        height=180,\n",
    "        width=180,\n",
    "        margin=dict(l=0, r=0, t=0, b=0),\n",
    "        paper_bgcolor=\"rgba(0,0,0,0)\",\n",
    "        hovermode=False,\n",
    "        modebar=None,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Pie(\n",
    "            name=\"gauge\",\n",
    "            values=pie_values,\n",
    "            marker=dict(\n",
    "                colors=marker_colors,\n",
    "                line=dict(width=4, color=\"#fafafa\"),\n",
    "            ),\n",
    "            hole=0.75,\n",
    "            direction=\"clockwise\",\n",
    "            sort=False,\n",
    "            rotation=234,\n",
    "            showlegend=False,\n",
    "            hoverinfo=\"none\",\n",
    "            textinfo=\"none\",\n",
    "            textposition=\"outside\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if score is not None:\n",
    "        if display_score:\n",
    "            fig.add_trace(\n",
    "                go.Indicator(\n",
    "                    mode=\"number\", value=score, domain=dict(x=[0, 1], y=[0.28, 0.45])\n",
    "                )\n",
    "            )\n",
    "        fig.add_shape(\n",
    "            type=\"circle\", fillcolor=\"black\", x0=0.475, x1=0.525, y0=0.475, y1=0.525\n",
    "        )\n",
    "        fig.add_shape(\n",
    "            type=\"path\",\n",
    "            fillcolor=\"black\",\n",
    "            line=dict(width=0),\n",
    "            path=_generate_pointer_path(score),\n",
    "        )\n",
    "\n",
    "    return fig\n",
    "\n",
    "def create_gauge_chart(score: int) -> Image:\n",
    "    fig = gauge_and_needle_chart(score)\n",
    "    fig.update_layout(\n",
    "        paper_bgcolor='rgba(0,0,0,0)',\n",
    "        plot_bgcolor='rgba(0,0,0,0)',\n",
    "        margin=dict(t=20, b=20, l=20, r=20)\n",
    "    )\n",
    "    img_bytes = to_image(fig, format=\"png\", scale=2)\n",
    "    img = PILImage.open(io.BytesIO(img_bytes))\n",
    "    img_buffer = io.BytesIO()\n",
    "    img.save(img_buffer, format=\"PNG\")\n",
    "    img_buffer.seek(0)\n",
    "    return Image(img_buffer, width=1.75*inch, height=1.75*inch)\n",
    "\n",
    "def create_report_pdf(data: Dict[str, Any], dataset_df: pd.DataFrame, output_filename: str = 'enhanced_data_quality_report.pdf'):\n",
    "    doc = SimpleDocTemplate(output_filename, pagesize=letter)\n",
    "    styles = getSampleStyleSheet()\n",
    "    \n",
    "    chart_title_style = ParagraphStyle(\n",
    "        name='ChartTitle', \n",
    "        parent=styles['BodyText'], \n",
    "        alignment=TA_CENTER,\n",
    "        fontSize=8,\n",
    "        leading=10\n",
    "    )\n",
    "    styles.add(chart_title_style)\n",
    "\n",
    "    styles['Title'].fontSize = 24\n",
    "    styles['Title'].alignment = 1\n",
    "    styles['Title'].spaceAfter = 12\n",
    "    styles['Title'].textColor = colors.HexColor('#110420')\n",
    "\n",
    "    styles['Heading1'].fontSize = 18\n",
    "    styles['Heading1'].spaceAfter = 6\n",
    "    styles['Heading1'].textColor = colors.HexColor('#110420')\n",
    "\n",
    "    styles['Heading2'].fontSize = 14\n",
    "    styles['Heading2'].spaceBefore = 12\n",
    "    styles['Heading2'].spaceAfter = 6\n",
    "    styles['Heading2'].textColor = colors.HexColor('#110420')\n",
    "\n",
    "    styles['BodyText'].fontSize = 10\n",
    "    styles['BodyText'].spaceBefore = 6\n",
    "    styles['BodyText'].spaceAfter = 6\n",
    "    styles['BodyText'].textColor = colors.HexColor('#110420')\n",
    "\n",
    "    styles.add(ParagraphStyle(name='RecordPreview',\n",
    "                              parent=styles['BodyText'],\n",
    "                              fontName='Courier',\n",
    "                              fontSize=8,\n",
    "                              leading=10,\n",
    "                              spaceAfter=12,\n",
    "                              firstLineIndent=0,\n",
    "                              leftIndent=20))\n",
    "    \n",
    "    story = []\n",
    "\n",
    "    data_completeness = (1 - dataset_df.isnull().sum().sum() / (dataset_df.shape[0] * dataset_df.shape[1])) * 100\n",
    "    \n",
    "    text_diversity_scores = []\n",
    "    gini_scores = []\n",
    "    if 'feature_distribution' in data and 'score' in data['feature_distribution']:\n",
    "        for value in data['feature_distribution']['score'].values():\n",
    "            if isinstance(value, dict):\n",
    "                if 'text_diversity_index' in value:\n",
    "                    text_diversity_scores.append(value['text_diversity_index'])\n",
    "                if 'gini_simpson_index' in value:\n",
    "                    gini_scores.append(value['gini_simpson_index'])\n",
    "    \n",
    "    avg_text_diversity = sum(text_diversity_scores) / len(text_diversity_scores) if text_diversity_scores else 0\n",
    "    avg_gini_index = sum(gini_scores) / len(gini_scores) if gini_scores else 0\n",
    "\n",
    "    story.append(Paragraph(\"Data Quality Report\", styles['Title']))\n",
    "    story.append(Spacer(1, 0.2*inch))\n",
    "\n",
    "    story.append(Paragraph(\"Key Metrics\", styles['Heading1']))\n",
    "    \n",
    "    unique_rows_chart = create_gauge_chart(int(data['row_uniqueness']['percent_unique']))\n",
    "    semantically_unique_rows_chart = create_gauge_chart(int(data['row_uniqueness']['percent_semantically_unique']))\n",
    "    text_diversity_chart = create_gauge_chart(int(avg_text_diversity * 100))\n",
    "    gini_simpson_chart = create_gauge_chart(int(avg_gini_index * 100))\n",
    "\n",
    "    unique_rows_title = Paragraph(\"Unique Rows\", styles['ChartTitle'])\n",
    "    semantically_unique_rows_title = Paragraph(\"Semantically Unique Rows\", styles['ChartTitle'])\n",
    "    text_diversity_title = Paragraph(\"Text Diversity\", styles['ChartTitle'])\n",
    "    gini_simpson_title = Paragraph(\"Gini-Simpson Index\", styles['ChartTitle'])\n",
    "\n",
    "    chart_table = Table([\n",
    "        [unique_rows_title, semantically_unique_rows_title, text_diversity_title, gini_simpson_title],\n",
    "        [unique_rows_chart, semantically_unique_rows_chart, text_diversity_chart, gini_simpson_chart]\n",
    "    ])\n",
    "    chart_table_style = TableStyle([\n",
    "        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
    "        ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),\n",
    "        ('BOTTOMPADDING', (0, 0), (-1, 0), 3), \n",
    "        ('TOPPADDING', (0, 1), (-1, -1), 3),\n",
    "    ])\n",
    "    chart_table.setStyle(chart_table_style)\n",
    "\n",
    "    story.append(chart_table)\n",
    "    story.append(Paragraph(\"‚Ä¢ Unique Rows: percentage of rows that are unique in the dataset.\", styles['BodyText']))\n",
    "    story.append(Paragraph(\"‚Ä¢ Semantically Unique Rows: percentage of rows that are semantically unique, based on TF-IDF\", styles['BodyText']))\n",
    "    story.append(Paragraph(\"‚Ä¢ Text Diversity: avg text diversity across all text fields, with higher values indicating more diverse content.\", styles['BodyText']))\n",
    "    story.append(Paragraph(\"‚Ä¢ Gini-Simpson Index: probability that two randomly selected entities from the dataset will be different, with higher values indicating greater diversity.\", styles['BodyText']))\n",
    "    story.append(Spacer(1, 0.2*inch))\n",
    "\n",
    "    story.append(Paragraph(\"Machine Learning Implications\", styles['Heading1']))\n",
    "    story.append(Paragraph(\"Key metrics above and analyses that follow have important implications for AI/ML use-cases. See the conclusion section for <b>General Considerations</b>, <b>Pre-training</b>, <b>Fine-tuning</b>, and <b>Designing/Iterating on Data to Fill Data Gaps</b>\", styles['BodyText']))\n",
    "    story.append(PageBreak())\n",
    "\n",
    "    # Dataset Overview\n",
    "    story.append(Paragraph(\"Dataset Overview\", styles['Heading1']))\n",
    "    story.append(Paragraph(\"This section provides key metrics on the structure, uniqueness, complexity, and quality of the data:\", styles['BodyText']))\n",
    "\n",
    "    overview_data = [\n",
    "        [\"Metric\", \"Value\"],\n",
    "        [\"Number of Rows\", f\"{len(dataset_df)}\"],\n",
    "        [\"Number of Columns\", f\"{len(dataset_df.columns)}\"],\n",
    "        [\"Categorical Columns\", f\"{len(dataset_df.select_dtypes(include=['object', 'category']).columns)}\"],\n",
    "        [\"Numerical Columns\", f\"{len(dataset_df.select_dtypes(include=['int64', 'float64']).columns)}\"],\n",
    "        [\"Data Completeness\", f\"{data_completeness:.2f}%\"],\n",
    "        [\"Unique Rows\", f\"{data['row_uniqueness']['percent_unique']}%\"],\n",
    "        [\"Semantically Unique Rows\", f\"{data['row_uniqueness']['percent_semantically_unique']}%\"],\n",
    "        [\"Avg Words per Record\", f\"{data['num_words_per_record']['average_words_per_record']:.2f}\"],\n",
    "        [\"Avg Tokens per Record\", \"N/A\"],\n",
    "        [\"Total Tokens\", \"N/A\"],\n",
    "        [\"Avg Text Diversity\", f\"{avg_text_diversity:.4f}\"],\n",
    "        [\"Avg Gini-Simpson Index\", f\"{avg_gini_index:.4f}\"],\n",
    "    ]\n",
    "\n",
    "    overview_table = create_overview_table(overview_data)\n",
    "    story.append(overview_table)\n",
    "    story.append(Spacer(1, 0.2*inch))\n",
    "    \n",
    "    story.append(Paragraph(\"The enhanced dataset overview provides key metrics about the structure, uniqueness, complexity, and quality of the data:\", styles['BodyText']))\n",
    "    story.append(Paragraph(\"‚Ä¢ Number of Rows and Columns: Indicates the size and dimensionality of the dataset.\", styles['BodyText']))\n",
    "    story.append(Paragraph(\"‚Ä¢ Categorical and Numerical Columns: Gives insight into the types of data present, helping to guide appropriate analysis techniques.\", styles['BodyText']))\n",
    "    story.append(Paragraph(\"‚Ä¢ Data Completeness: Shows the overall percentage of non-null values across all fields, indicating the dataset's overall quality and potential need for imputation.\", styles['BodyText']))\n",
    "    story.append(Paragraph(\"‚Ä¢ Unique and Semantically Unique Rows: Demonstrates the level of data diversity and potential redundancy in the dataset.\", styles['BodyText']))\n",
    "    story.append(Paragraph(\"‚Ä¢ Average Words per Record: Provides an indication of the typical complexity or detail level of each entry.\", styles['BodyText']))\n",
    "    story.append(Paragraph(\"‚Ä¢ Average Tokens per Record and Total Tokens: These metrics correspond to tokens used in Large Language Models (LLMs), giving an estimate of the dataset's complexity from an LLM processing perspective.\", styles['BodyText']))\n",
    "    story.append(Paragraph(\"‚Ä¢ Average Text Diversity: Quantifies the variety of content in text fields, with higher values indicating more diverse text data.\", styles['BodyText']))\n",
    "    story.append(Paragraph(\"‚Ä¢ Average Gini-Simpson Index: Represents the probability that two entities taken at random from the dataset (with replacement) represent different types. Higher values indicate greater diversity.\", styles['BodyText']))\n",
    "    story.append(Spacer(1, 0.2*inch))\n",
    "    story.append(PageBreak())\n",
    "\n",
    "    \n",
    "    # Dataset Schema\n",
    "    story.append(Paragraph(\"Dataset Schema & Preview\", styles['Heading1']))\n",
    "    schema_table, avg_lengths = create_schema_table(dataset_df, data)\n",
    "    story.append(schema_table)\n",
    "    story.append(Spacer(1, 0.2*inch))\n",
    "    story.append(Paragraph(\"The schema table provides an overview of each column in the dataset, including the data type, the count of non-null and null values, and the average length (where applicable). This information is crucial for understanding the structure of the data and identifying potential data quality issues such as missing values or unexpected data types.\", styles['BodyText']))\n",
    "    story.append(Spacer(1, 0.2*inch))\n",
    "\n",
    "    # Single Record Preview\n",
    "    story.append(Paragraph(\"Single Record Preview\", styles['Heading2']))\n",
    "    preview_text = create_single_record_preview(dataset_df)\n",
    "    story.append(Paragraph(preview_text, styles['RecordPreview']))\n",
    "    story.append(Spacer(1, 0.2*inch))\n",
    "    story.append(PageBreak())\n",
    "\n",
    "\n",
    "    # Feature Cardinality\n",
    "    if 'feature_cardinality' in data:\n",
    "        story.append(Paragraph(\"Feature Cardinality\", styles['Heading1']))\n",
    "        feature_cardinality = pd.DataFrame.from_dict(data['feature_cardinality'], orient='index', columns=['cardinality'])\n",
    "        \n",
    "        img = create_chart(feature_cardinality['cardinality'], \"Feature Cardinality\", \"Features\", \"Cardinality\")\n",
    "        story.append(img)\n",
    "        \n",
    "        story.append(Paragraph(\"Feature cardinality represents the number of unique values for each feature in the dataset. Higher cardinality indicates more diverse values within a feature, which can be both beneficial and challenging for analysis:\", styles['BodyText']))\n",
    "        story.append(Paragraph(\"‚Ä¢ High cardinality features (e.g., 'id' with 1000 unique values) often represent unique identifiers or very specific attributes. These can be useful for individual record identification but may not be as useful for broader pattern analysis.\", styles['BodyText']))\n",
    "        story.append(Paragraph(\"‚Ä¢ Medium cardinality features (e.g., 'domain' and 'domain_description' with 100 unique values each) often represent categorical data with a moderate number of categories. These can be valuable for grouping and analyzing trends across different categories.\", styles['BodyText']))\n",
    "        story.append(Paragraph(\"‚Ä¢ Low cardinality features (e.g., 'sql_complexity' and 'sql_complexity_description' with 8 unique values each) typically represent broader categories or classifications. These can be particularly useful for high-level grouping and analysis.\", styles['BodyText']))\n",
    "        story.append(Spacer(1, 0.2*inch))\n",
    "        story.append(PageBreak())\n",
    "\n",
    "    # Distribution Visualizations\n",
    "    if 'feature_distribution' in data and 'distribution' in data['feature_distribution']:\n",
    "        for key, distribution in data['feature_distribution']['distribution'].items():\n",
    "            if distribution and isinstance(distribution, dict):\n",
    "                try:\n",
    "                    story.append(Paragraph(f\"{key.replace('_', ' ').title()} Distribution\", styles['Heading1']))\n",
    "                    dist_df = pd.DataFrame.from_dict(distribution, orient='index', columns=['count'])\n",
    "                    dist_df['count'] = pd.to_numeric(dist_df['count'], errors='coerce')\n",
    "                    dist_df = dist_df.dropna().sort_values('count', ascending=False)\n",
    "                    \n",
    "                    if not dist_df.empty:\n",
    "                        # Handle large distributions\n",
    "                        if len(dist_df) > 75:\n",
    "                            other_count = dist_df.iloc[75:]['count'].sum()\n",
    "                            dist_df = dist_df.iloc[:75]\n",
    "                            dist_df.loc['Other'] = other_count\n",
    "\n",
    "                        img = create_pareto_chart(dist_df, f\"{key.replace('_', ' ').title()} Distribution (Pareto Chart)\")\n",
    "                        story.append(img)\n",
    "                        \n",
    "                        content = f\"This Pareto chart illustrates the distribution of {key.replace('_', ' ')} in the dataset. The bars represent the count for each category, while the line shows the cumulative percentage. This visualization helps identify the most significant categories and their relative importance:\"\n",
    "                        story.append(Paragraph(content, styles['BodyText']))\n",
    "                        story.append(Paragraph(f\"‚Ä¢ The most common {key.replace('_', ' ')} is '{dist_df.index[0]}' with a count of {dist_df['count'].iloc[0]}.\", styles['BodyText']))\n",
    "                        story.append(Paragraph(f\"‚Ä¢ The top 3 categories account for {100 * dist_df['count'].iloc[:3].sum() / dist_df['count'].sum():.1f}% of all entries.\", styles['BodyText']))\n",
    "                        if len(dist_df) > 75:\n",
    "                            story.append(Paragraph(f\"‚Ä¢ Due to the large number of categories, only the top 75 are shown individually. The remaining categories are grouped as 'Other'.\", styles['BodyText']))\n",
    "                        story.append(Paragraph(\"‚Ä¢ This distribution can help in understanding the focus areas of the dataset and potential biases in the data collection or generation process.\", styles['BodyText']))\n",
    "                        \n",
    "                        if 'score' in data['feature_distribution'] and key in data['feature_distribution']['score']:\n",
    "                            story.append(Paragraph(\"Diversity scores:\", styles['BodyText']))\n",
    "                            for score_key, score_value in data['feature_distribution']['score'][key].items():\n",
    "                                story.append(Paragraph(f\"‚Ä¢ {score_key.replace('_', ' ').title()}: {score_value:.4f}\", styles['BodyText']))\n",
    "                            story.append(Paragraph(\"These scores provide a quantitative measure of the diversity within this feature. Higher scores indicate a more even distribution across categories, which can be beneficial for comprehensive analysis but may also present challenges in identifying dominant trends.\", styles['BodyText']))\n",
    "                        \n",
    "                        story.append(Spacer(1, 0.2*inch))\n",
    "                        story.append(PageBreak())\n",
    "                except Exception as e:\n",
    "                    story.append(Paragraph(f\"Error processing {key} distribution: {str(e)}\", styles['BodyText']))\n",
    "\n",
    "    # Word Count per Column\n",
    "    if 'word_counts_per_column' in data['num_words_per_record']:\n",
    "        story.append(Paragraph(\"Average Word Count per Column\", styles['Heading1']))\n",
    "        word_count = pd.DataFrame.from_dict(data['num_words_per_record']['word_counts_per_column'], orient='index', columns=['avg_words'])\n",
    "        word_count = word_count.sort_values('avg_words', ascending=False)\n",
    "        \n",
    "        img = create_chart(word_count['avg_words'], \"Average Word Count per Column\", \"Columns\", \"Average Word Count\")\n",
    "        story.append(img)\n",
    "        \n",
    "        story.append(Paragraph(\"This chart displays the average word count for each column in the dataset, providing insights into the verbosity of different features:\", styles['BodyText']))\n",
    "        story.append(Paragraph(f\"‚Ä¢ The most verbose column is '{word_count.index[0]}' with an average of {word_count['avg_words'].iloc[0]:.2f} words.\", styles['BodyText']))\n",
    "        story.append(Paragraph(f\"‚Ä¢ The least verbose column is '{word_count.index[-1]}' with an average of {word_count['avg_words'].iloc[-1]:.2f} words.\", styles['BodyText']))\n",
    "        story.append(Paragraph(\"‚Ä¢ Columns with higher word counts may contain more detailed information or longer descriptions. These could be prime targets for text analysis or summarization tasks.\", styles['BodyText']))\n",
    "        story.append(Paragraph(\"‚Ä¢ Columns with lower word counts might represent more categorical or numerical data, which could be suitable for different types of analysis such as classification or regression tasks.\", styles['BodyText']))\n",
    "        story.append(Spacer(1, 0.2*inch))\n",
    "        story.append(PageBreak())\n",
    "\n",
    "    # Text Diversity Indices\n",
    "    if 'feature_distribution' in data and 'score' in data['feature_distribution']:\n",
    "        text_diversity = {}\n",
    "        for key, value in data['feature_distribution']['score'].items():\n",
    "            if isinstance(value, dict) and 'text_diversity_index' in value:\n",
    "                text_diversity[key] = value['text_diversity_index']\n",
    "        if text_diversity:\n",
    "            story.append(Paragraph(\"Text Diversity Indices\", styles['Heading1']))\n",
    "            text_diversity_df = pd.DataFrame.from_dict(text_diversity, orient='index', columns=['diversity_index'])\n",
    "            img = create_text_diversity_chart(text_diversity_df)\n",
    "            story.append(img)\n",
    "            \n",
    "            story.append(Paragraph(\"Text diversity indices provide a measure of how varied the content is within different text fields:\", styles['BodyText']))\n",
    "            story.append(Paragraph(f\"‚Ä¢ The highest diversity is found in the '{text_diversity_df['diversity_index'].idxmax()}' column with an index of {text_diversity_df['diversity_index'].max():.4f}.\", styles['BodyText']))\n",
    "            story.append(Paragraph(f\"‚Ä¢ The lowest diversity is in the '{text_diversity_df['diversity_index'].idxmin()}' column with an index of {text_diversity_df['diversity_index'].min():.4f}.\", styles['BodyText']))\n",
    "            story.append(Paragraph(\"‚Ä¢ Higher values suggest more diverse content, which can be beneficial for tasks requiring a wide range of examples or training data.\", styles['BodyText']))\n",
    "            story.append(Paragraph(\"‚Ä¢ Lower values might indicate more standardized or repetitive content, which could be easier to process but may provide less varied information.\", styles['BodyText']))\n",
    "            story.append(Spacer(1, 0.2*inch))\n",
    "            story.append(PageBreak())\n",
    "\n",
    "    # Conclusion\n",
    "    story.append(Paragraph(\"Conclusion\", styles['Heading1']))\n",
    "\n",
    "    conclusion_text = \"This report provides a comprehensive view of the dataset's structure, content diversity, and the nature of the data it contains. Key takeaways include:<br/>\"\n",
    "\n",
    "    # Data Uniqueness\n",
    "    if 'row_uniqueness' in data:\n",
    "        unique = data['row_uniqueness'].get('percent_unique', 'N/A')\n",
    "        sem_unique = data['row_uniqueness'].get('percent_semantically_unique', 'N/A')\n",
    "        conclusion_text += f\"1. Data Uniqueness: With {unique}% unique rows and {sem_unique}% semantically unique rows, \"\n",
    "        if unique != 'N/A' and float(unique) > 90:\n",
    "            conclusion_text += \"the dataset shows a high degree of individuality in its records. This suggests a rich and varied dataset.<br/><br/>\"\n",
    "        else:\n",
    "            conclusion_text += \"the dataset shows some level of repetition in its records. This may indicate patterns or recurring themes in the data.<br/><br/>\"\n",
    "\n",
    "    # Feature Cardinality\n",
    "    if 'feature_cardinality' in data:\n",
    "        conclusion_text += \"2. Feature Cardinality: The dataset contains features with varying cardinalities. \"\n",
    "        conclusion_text += \"This diversity in feature types allows for both granular analysis and higher-level pattern recognition.<br/><br/>\"\n",
    "\n",
    "    # Distribution Patterns\n",
    "    if 'feature_distribution' in data and 'distribution' in data['feature_distribution']:\n",
    "        conclusion_text += \"3. Distribution Patterns: The charts reveal the distribution patterns within each feature, \"\n",
    "        conclusion_text += \"highlighting potential focus areas or biases in the data. Understanding these distributions \"\n",
    "        conclusion_text += \"is crucial for balanced analysis and identifying underrepresented categories.<br/><br/>\"\n",
    "\n",
    "    # Text Complexity\n",
    "    if 'num_words_per_record' in data:\n",
    "        avg_words = data['num_words_per_record'].get('average_words_per_record', 'N/A')\n",
    "        if avg_words != 'N/A':\n",
    "            conclusion_text += f\"4. Text Complexity: With an average of {avg_words:.2f} words per record, \"\n",
    "            if float(avg_words) > 50:\n",
    "                conclusion_text += \"the dataset shows a high level of complexity. \"\n",
    "            elif float(avg_words) > 20:\n",
    "                conclusion_text += \"the dataset shows a moderate level of complexity. \"\n",
    "            else:\n",
    "                conclusion_text += \"the dataset shows a low level of complexity. \"\n",
    "            conclusion_text += \"This gives an indication of the depth of information contained in each record.<br/><br/>\"\n",
    "\n",
    "    # Text Diversity\n",
    "    if 'feature_distribution' in data and 'score' in data['feature_distribution']:\n",
    "        conclusion_text += \"5. Text Diversity: The text diversity indices provide insight into the variety of content within text fields. \"\n",
    "        conclusion_text += \"Higher diversity can be beneficial for tasks requiring a broad range of examples, while lower diversity \"\n",
    "        conclusion_text += \"might indicate more standardized content.<br/><br/>\"\n",
    "\n",
    "    conclusion_text += \"\"\"\n",
    "    Implications for Machine Learning:<br/><br/> \n",
    " \n",
    "    <b>Pre-training</b><br/> \n",
    "    - The dataset's uniqueness and diversity can provide a rich foundation for pre-training language models or other AI systems.<br/>\n",
    "    - High cardinality features may help in learning broad representations, while low cardinality features could aid in learning important categorical distinctions.<br/>\n",
    "    - If text diversity is high, it could be particularly valuable for building robust language models that can handle a wide range of contexts and styles.<br/><br/>\n",
    "\n",
    "    <b>Fine-tuning:</b><br/> \n",
    "    - The distribution patterns revealed in the charts should guide the fine-tuning process. Imbalanced categories may require techniques like weighted sampling or loss adjustment to ensure equal representation during fine-tuning.<br/>\n",
    "    - Features with high semantic uniqueness could be especially useful for fine-tuning models on specific domains or tasks, as they likely contain a wide range of relevant examples.<br/>\n",
    "    - Consider the average word count per record when deciding on sequence length for transformer-based models during fine-tuning.<br/><br/>\n",
    "\n",
    "    <b>Designing/Iterating on Data to Fill Data Gaps:</b><br/> \n",
    "    - Analyze the distribution charts to identify underrepresented categories. These areas may require additional data collection or augmentation to ensure comprehensive model performance.<br/>\n",
    "    - If certain text diversity scores are low, consider ways to introduce more variety in those fields, either through data augmentation techniques or targeted data collection.<br/>\n",
    "    - For features with very high cardinality, consider if grouping or categorization might be beneficial to prevent overfitting on rare categories.<br/>\n",
    "    - If semantic uniqueness is low in certain areas, it might indicate a need for more diverse examples in those categories to improve model generalization.<br/><br/>\n",
    "\n",
    "    <b>General Considerations:</b><br/> \n",
    "    - The overall uniqueness of the dataset impacts models that require diverse examples. However, care should be taken to address any imbalances revealed in the distribution charts.<br/>\n",
    "    - Monitor for potential biases in the data that could be propagated or amplified by machine learning models.<br/>\n",
    "    - Consider privacy implications, especially for high-cardinality features that might contain identifiable information.<br/>\n",
    "    - The text complexity (average words per record) should inform decisions about model architecture and preprocessing steps.<br/><br/>\n",
    "    \"\"\"\n",
    "\n",
    "    story.append(Paragraph(conclusion_text, styles['BodyText']))\n",
    "    \n",
    "    # Build the PDF\n",
    "    doc.build(story)\n",
    "    print(f\"PDF created: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report_pdf(results, dataset_df,  'data_quality_report.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nav-helpers",
   "language": "python",
   "name": "nav-helpers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
