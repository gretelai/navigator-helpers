{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: need to install psycopg2 from source if using in production environment\n",
    "# https://www.psycopg.org/docs/install.html\n",
    "# %pip install sqlglot sqlvalidator sqlalchemy psycopg2-binary sqlfluff mysql-connector-python pyodbc google-cloud-bigquery\n",
    "# %pip install pyflakes pylint parso flake8 mypy ruff\n",
    "# %pip install docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import sql_parsers\n",
    "reload(sql_parsers)\n",
    "\n",
    "import python_parsers\n",
    "reload(python_parsers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docker\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from python_parsers import *\n",
    "from sql_parsers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_queries = pd.read_csv('/mnt/foundation-shared/nina_xu_gretel_ai/datasets/sql_queries_w_dialect_1000.csv')\n",
    "sql_queries_googlesql = pd.read_csv('/mnt/foundation-shared/nina_xu_gretel_ai/datasets/sql_queries_googlesql_200.csv')\n",
    "sql_queries = pd.concat([sql_queries, sql_queries_googlesql], ignore_index=True)\n",
    "python_typscript_codes = pd.read_csv('/mnt/foundation-shared/nina_xu_gretel_ai/datasets/python_typescript_codes.csv')\n",
    "python_codes = pd.read_json('/mnt/foundation-shared/nina_xu_gretel_ai/datasets/text_to_python_v1.json')\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Give each row a unique db name because otherwise BigQuery struggles with the same db name\n",
    "sql_queries['id_tmp'] = sql_queries.index\n",
    "sql_queries['db_name'] = sql_queries.apply(lambda x: f\"db_{x.id_tmp}\", axis=1)\n",
    "\n",
    "# Basic cleaning. BigQuery errors out if there are newlines like 'CREATE\\nTABLE'\n",
    "sql_queries['SQL Query'] = sql_queries['SQL Query'].apply(\n",
    "    lambda x: x.replace('CREATE\\nTABLE', 'CREATE TABLE').replace('CREATE\\nVIEW', 'CREATE VIEW').replace('INSERT\\nINTO', 'INSERT INTO'))\n",
    "sql_queries['Context'] = sql_queries['Context'].apply(\n",
    "    lambda x: x.replace('CREATE\\nTABLE', 'CREATE TABLE').replace('CREATE\\nVIEW', 'CREATE VIEW').replace('INSERT\\nINTO', 'INSERT INTO'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Code Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_queries.head(1)\n",
    "print(sql_queries.Dialect.value_counts())\n",
    "\n",
    "sql_queries.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_queries.Complexity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Have a PostgreSQL database running in a Docker container. In command line, run the following commands:\n",
    "# Grant access to non-root users so that the python client will work\n",
    "> sudo groupadd docker\n",
    "> sudo usermod -aG docker $USER\n",
    "> newgrp docker\n",
    "\n",
    "> docker pull postgres\n",
    "> docker run --name my-postgres \\\n",
    "  -e POSTGRES_USER=myuser \\\n",
    "  -e POSTGRES_PASSWORD=mypassword \\\n",
    "  -e POSTGRES_DB=mydatabase \\\n",
    "  -p 5433:5432 \\\n",
    "  -d postgres\n",
    "\n",
    "\"\"\"\n",
    "client = docker.from_env()\n",
    "\n",
    "# List all running containers\n",
    "containers = client.containers.list(all=False)\n",
    "# Get the postgres container\n",
    "postgres_container = client.containers.get('my-postgres')\n",
    "# Get container's gateway, not that it's not the \"IPAddress\" field\n",
    "postgres_container_gateway = postgres_container.attrs['NetworkSettings']['Gateway']\n",
    "print(postgres_container_gateway)\n",
    "\n",
    "postgres_db_creds = {\n",
    "        \"host\": postgres_container_gateway,\n",
    "        \"port\": 5433, # the default port is 5432, but that was already in use for me\n",
    "        \"user\": \"myuser\",\n",
    "        \"password\": \"mypassword\",\n",
    "        \"dbname\": \"my-postgres\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Have a MySQL database running in a Docker container. In command line, run the following commands:\n",
    "> docker pull mysql\n",
    "> docker run --name my-mysql \\\n",
    "  -e MYSQL_ROOT_PASSWORD=myrootpassword \\\n",
    "  -d mysql\n",
    "\"\"\"\n",
    "\n",
    "mysql_container = client.containers.get('my-mysql')\n",
    "mysql_container_ip = mysql_container.attrs['NetworkSettings']['IPAddress']\n",
    "print(mysql_container_ip)\n",
    "\n",
    "mysql_db_creds = {\n",
    "    \"host\": mysql_container_ip,\n",
    "    \"port\": 3306, # default port for mysql\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"myrootpassword\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Have a Microsoft SQL Server database running in a Docker container. In command line, run the following commands:\n",
    "$ docker pull mcr.microsoft.com/mssql/server\n",
    "$ docker run --name my-sqlserver \\\n",
    "  -e 'ACCEPT_EULA=Y' -e 'MSSQL_SA_PASSWORD=myRoot(!)Password' \\\n",
    "  -p 1433:1433 \\\n",
    "  -d mcr.microsoft.com/mssql/server\n",
    "\n",
    "$ sudo apt install unixodbc-dev\n",
    "\n",
    "Install the SQL Server command-line tool (sqlcmd) inside the container:\n",
    "$ docker exec -it --user root my-sqlserver bash\n",
    "# apt-get update\n",
    "# apt-get install -y mssql-tools unixodbc-dev\n",
    "\"\"\"\n",
    "          \n",
    "sqlserver_container = client.containers.get('my-sqlserver')\n",
    "sqlserver_container_ip = sqlserver_container.attrs['NetworkSettings']['IPAddress']\n",
    "print(sqlserver_container_ip)\n",
    "\n",
    "sqlserver_db_creds = {\n",
    "    \"host\": sqlserver_container_ip,\n",
    "    \"port\": 1433, # default port for sql server,\n",
    "    \"user\": \"sa\",\n",
    "    \"password\": \"myRoot(!)Password\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Have a BigQuery emulator running in a Docker container. The official BigQuery image requires authentication \n",
    "to Google Cloud and would actually interact with BigQuery. In command line, run the following commands:\n",
    "\n",
    "$ docker pull ghcr.io/goccy/bigquery-emulator:latest\n",
    "$ docker run -it -p 9050:9050 ghcr.io/goccy/bigquery-emulator:latest --project=test-project\n",
    "\n",
    "Note: if running the same SQL queries again, kill the container and start a fresh one because \n",
    "the deleting dataset functionality was not working as expected.\n",
    "\"\"\"\n",
    "\n",
    "biquery_db_creds = {\n",
    "     \"port\": 9050,\n",
    "     \"project\": \"test-project\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Apply different SQL validators to the SQL queries\n",
    "def is_valid_query_and_schema(row, func):\n",
    "    query_check = func(row['SQL Query'])\n",
    "    schema_check = func(row['Context'])\n",
    "    is_valid_schema = schema_check[0]\n",
    "    is_valid_query = query_check[0]\n",
    "    is_valid_sql = is_valid_schema and is_valid_query\n",
    "    error_messages = f\"***Schema error: {schema_check[1]}\" if not is_valid_schema else ''\n",
    "    error_messages += f\"***Query error: {query_check[1]}\" if not is_valid_query else ''\n",
    "    return is_valid_sql, is_valid_schema, is_valid_query, error_messages\n",
    "\n",
    "def is_valid_query_and_schema_with_sqlfluff(row):\n",
    "    dialect_map = {\n",
    "        'SQLite': 'sqlite',\n",
    "        'PostgreSQL': 'postgres',\n",
    "        'MySQL': 'mysql',\n",
    "        'SQL Server': 'tsql',\n",
    "        'GoogleSQL': 'bigquery',\n",
    "        'Oracle': 'oracle',\n",
    "    }\n",
    "    if 'Oracle' in row['Dialect']:\n",
    "        dialect = 'oracle'\n",
    "    else:\n",
    "        dialect = dialect_map.get(row['Dialect'], 'ansi')\n",
    "    query_check = SimpleSqlValidator.is_valid_sql_with_sqlfluff(row['SQL Query'], dialect)\n",
    "    schema_check = SimpleSqlValidator.is_valid_sql_with_sqlfluff(row['Context'], dialect)\n",
    "    is_valid_schema = schema_check[0]\n",
    "    is_valid_query = query_check[0]\n",
    "    is_valid_sql = is_valid_schema and is_valid_query\n",
    "    error_messages = f\"***Schema error: {schema_check[1]}\" if not is_valid_schema else ''\n",
    "    error_messages += f\"***Query error: {query_check[1]}\" if not is_valid_query else ''\n",
    "    return is_valid_sql, is_valid_schema, is_valid_query, error_messages\n",
    "\n",
    "def check_query_and_schema_separately(sql_queries, method):\n",
    "    start_time = time.time()\n",
    "    functions_to_apply = {\n",
    "        'sqlglot': partial(is_valid_query_and_schema, func=SimpleSqlValidator.is_valid_sql_with_sqlglot),\n",
    "        'sqlquery': partial(is_valid_query_and_schema, func=SimpleSqlValidator.is_valid_sql_with_sqlquery),\n",
    "        'sqlfluff': is_valid_query_and_schema_with_sqlfluff,\n",
    "    }\n",
    "\n",
    "    result = sql_queries.apply(functions_to_apply[method], axis=1).apply(list)\n",
    "    sql_queries[f'is_valid_sql_with_{method}'] = result.apply(lambda x: x[0])\n",
    "    sql_queries[f'is_valid_schema_with_{method}'] = result.apply(lambda x: x[1])\n",
    "    sql_queries[f'is_valid_query_with_{method}'] = result.apply(lambda x: x[2])\n",
    "    sql_queries[f'error_msgs_{method}'] = result.apply(lambda x: x[3])\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"{method} check executed in {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    return sql_queries\n",
    "\n",
    "\n",
    "def check_query_against_schema(row, dialect):\n",
    "\n",
    "    validator_classes = {\n",
    "        'SQLite': SqliteValidator,\n",
    "        'PostgreSQL': PostgresqlValidator,\n",
    "        'MySQL': MysqlValidator,\n",
    "        'SQL Server': SqlserverValidator,\n",
    "        'GoogleSQL': GooglesqlValidator,\n",
    "    }\n",
    "\n",
    "    kwargs_postgres = {\n",
    "        'domain': row['Topic'],\n",
    "        'db_creds': postgres_db_creds,\n",
    "    }\n",
    "    kwargs_mysql = {\n",
    "        'domain': row['Topic'],\n",
    "        'db_creds': mysql_db_creds,\n",
    "        'mysql_container': mysql_container,\n",
    "    }\n",
    "    kwargs_sqlserver = {\n",
    "        'domain': row['Topic'],\n",
    "        'db_creds': sqlserver_db_creds,\n",
    "        'sqlserver_container': sqlserver_container,\n",
    "    }\n",
    "    kwargs_bigquery = {\n",
    "        'domain': row['db_name'],\n",
    "        'db_creds': biquery_db_creds,\n",
    "    }\n",
    "\n",
    "    all_kwargs = {\n",
    "        'SQLite': {},\n",
    "        'PostgreSQL': kwargs_postgres,\n",
    "        'MySQL': kwargs_mysql,\n",
    "        'SQL Server': kwargs_sqlserver,\n",
    "        'GoogleSQL': kwargs_bigquery\n",
    "    }\n",
    "\n",
    "    dialect_name = dialect.lower().replace(' ', '')\n",
    "\n",
    "    if row['Dialect'] == dialect:\n",
    "        result = validator_classes[dialect].is_valid_sql(\n",
    "            row['SQL Query'], row['Context'], **all_kwargs[dialect]\n",
    "            )\n",
    "    else:\n",
    "        result = None, None\n",
    "    \n",
    "    row[f'is_valid_{dialect_name}'] = result[0]\n",
    "    row[f'error_msg_{dialect_name}'] = result[1]\n",
    "    \n",
    "    return row\n",
    "\n",
    "def apply_check_query_against_schema(sql_queries, dialect):\n",
    "    start_time = time.time()\n",
    "    sql_queries = sql_queries.apply(check_query_against_schema, dialect=dialect, axis=1)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"{dialect} check executed in {elapsed_time:.2f} seconds\")\n",
    "    return sql_queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_queries = check_query_and_schema_separately(sql_queries, 'sqlfluff')\n",
    "# sql_queries = check_query_and_schema_separately(sql_queries, 'sqlglot')\n",
    "# sql_queries = check_query_and_schema_separately(sql_queries, 'sqlquery')\n",
    "\n",
    "# sql_queries = apply_check_query_against_schema(sql_queries, 'SQLite')\n",
    "# sql_queries = apply_check_query_against_schema(sql_queries, 'PostgreSQL')\n",
    "# sql_queries = apply_check_query_against_schema(sql_queries, 'MySQL')\n",
    "# sql_queries = apply_check_query_against_schema(sql_queries, 'SQL Server')\n",
    "# sql_queries = apply_check_query_against_schema(sql_queries, 'GoogleSQL')\n",
    "\n",
    "print(sql_queries.is_valid_sql_with_sqlglot.value_counts(normalize=True))\n",
    "print(sql_queries.is_valid_sql_with_sqlquery.value_counts(normalize=True))\n",
    "print(sql_queries.is_valid_sql_with_sqlfluff.value_counts(normalize=True))\n",
    "\n",
    "print(sql_queries.is_valid_sqlite.value_counts(normalize=True))\n",
    "print(sql_queries.is_valid_postgresql.value_counts(normalize=True))\n",
    "print(sql_queries.is_valid_mysql.value_counts(normalize=True))\n",
    "print(sql_queries.is_valid_sqlserver.value_counts(normalize=True))\n",
    "print(sql_queries.is_valid_googlesql.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_googlesql_error_categories(error_msg):\n",
    "    if not error_msg:\n",
    "        return None\n",
    "    googlesql_error_categories = ['Type not found', 'Syntax error', 'Foreign keys are not supported', 'does not support']\n",
    "    for category in googlesql_error_categories:\n",
    "        if category.lower() in error_msg.lower():\n",
    "            return category\n",
    "\n",
    "\n",
    "sql_queries['googlesql_error_category'] = sql_queries['error_msg_googlesql'].apply(get_googlesql_error_categories)\n",
    "\n",
    "remaining = sql_queries[(sql_queries.is_valid_googlesql == False) & (sql_queries.googlesql_error_category.isnull())][['SQL Query', 'Context', 'error_msg_googlesql']]\n",
    "print(sql_queries.googlesql_error_category.value_counts())\n",
    "print(remaining.count())\n",
    "# remaining.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Coalesce the results of the five dialects into a single column\n",
    "sql_queries['is_valid_sql'] = sql_queries['is_valid_sqlite'].fillna(\n",
    "    sql_queries['is_valid_mysql'].fillna(\n",
    "        sql_queries['is_valid_postgresql'].fillna(\n",
    "            sql_queries['is_valid_sqlserver'].fillna(\n",
    "                sql_queries['is_valid_googlesql']\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "sql_queries['error_msg_sql'] = sql_queries['error_msg_sqlite'].fillna(\n",
    "    sql_queries['error_msg_mysql'].fillna(\n",
    "        sql_queries['error_msg_postgresql'].fillna(\n",
    "            sql_queries['error_msg_sqlserver'].fillna(\n",
    "                sql_queries['error_msg_googlesql']\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_queries.to_csv('/mnt/foundation-shared/nina_xu_gretel_ai/datasets/sqlqueries_1200_validated_092524.csv', index=False)\n",
    "# sql_queries = pd.read_csv('/mnt/foundation-shared/nina_xu_gretel_ai/datasets/sqlqueries_1200_validated_092524.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialects = ['SQLite', 'PostgreSQL', 'MySQL', 'SQL Server', 'GoogleSQL']\n",
    "for dialect in dialects:\n",
    "    print(f\"\\n***{dialect}***\")\n",
    "    print(sql_queries[sql_queries['Dialect'] == dialect].is_valid_sql_with_sqlglot.value_counts(normalize=True))\n",
    "    print(sql_queries[sql_queries['Dialect'] == dialect].is_valid_sql_with_sqlquery.value_counts(normalize=True))\n",
    "    print(sql_queries[sql_queries['Dialect'] == dialect].is_valid_sql_with_sqlfluff.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['sqlglot', 'sqlquery', 'sqlfluff']\n",
    "for dialect in dialects:\n",
    "    print(f\"\\n***{dialect}***\")\n",
    "    for method in methods:\n",
    "        print(f\"***{method}***\")\n",
    "        print(sql_queries[sql_queries['Dialect'] == dialect][f'is_valid_sql_with_{method}'].value_counts(normalize=True))\n",
    "        print(sql_queries[sql_queries['Dialect'] == dialect][f'is_valid_schema_with_{method}'].value_counts(normalize=True))\n",
    "        print(sql_queries[sql_queries['Dialect'] == dialect][f'is_valid_query_with_{method}'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the query is valid with both sqlglot and sqlfluff\n",
    "# SQLQuery is proven to be useless so not counting it in the aggregate\n",
    "sql_queries['is_valid_sql_aggregate'] = sql_queries[['is_valid_sql_with_sqlglot', 'is_valid_sql_with_sqlfluff']].all(axis=1)\n",
    "for dialect in dialects:\n",
    "    print(f\"***{dialect}***\")\n",
    "    print(sql_queries[sql_queries['Dialect'] == dialect].is_valid_sql_aggregate.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the differences between checking against schema and validating the query separately from schema?\n",
    "for dialect in dialects:\n",
    "    print(f\"\\n***{dialect}***\")\n",
    "    dialect_name = dialect.lower().replace(' ', '')\n",
    "    df = sql_queries[sql_queries['Dialect'] == dialect]\n",
    "    print(pd.crosstab(df[f'is_valid_{dialect_name}'], df['is_valid_sql_with_sqlfluff']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect = dialects[4]\n",
    "dialect_name = dialect.lower().replace(' ', '')\n",
    "print(dialect)\n",
    "df = sql_queries[(sql_queries['Dialect'] == dialect) & \n",
    "                 ((sql_queries['is_valid_sql_with_sqlfluff'] == False) & \n",
    "                  (sql_queries[f'is_valid_{dialect_name}'] == True))]\n",
    "df[['SQL Query', 'Context', f'error_msg_{dialect_name}', 'error_msgs_sqlfluff', 'error_msgs_sqlquery', 'error_msgs_sqlglot']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sql_queries['Context'].loc[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Code Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_check_methods = {\n",
    "    # 'compile': is_valid_python_with_complie,\n",
    "    # 'ast': is_valid_python_with_ast,\n",
    "    # 'pyflakes': is_valid_python_with_pyflakes,\n",
    "    # 'parso': is_valid_python_with_parso,\n",
    "    # 'mypy': is_valid_python_with_mypy,\n",
    "    'ruff': is_valid_python_with_ruff,\n",
    "    # 'ruff_extensive': is_valid_python_with_ruff,\n",
    "    # 'ruff_pyflakes': is_valid_python_with_ruff,\n",
    "    # 'pylint': is_valid_python_with_pylint,\n",
    "}\n",
    "\n",
    "def check_python_code_with_method(df, method='compile', **kwargs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    func = python_check_methods[method]\n",
    "    df[f'check_{method}'] = df['code'].apply(func, **kwargs)\n",
    "    df[f'is_valid_python_with_{method}'] = df[f'check_{method}'].apply(lambda x: x[0])\n",
    "    df[f'{method}_error'] = df[f'check_{method}'].apply(lambda x: x[1])\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"\\n{method} check executed in {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for method in python_check_methods.keys():\n",
    "    python_codes = check_python_code_with_method(python_codes, method)\n",
    "\n",
    "# python_codes = check_python_code_with_method(python_codes, 'ruff_extensive', level='warning')\n",
    "# ruff_pyflakes_args = {\n",
    "#     'level': 'custom',\n",
    "#     'ruff_rules': ['F'],\n",
    "# }\n",
    "# python_codes = check_python_code_with_method(python_codes, 'ruff_pyflakes', **ruff_pyflakes_args)\n",
    "\n",
    "for method in python_check_methods.keys():\n",
    "    print(python_codes[f'is_valid_python_with_{method}'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'pylint_error' in python_codes.columns:\n",
    "    python_codes['pylint_score'] = python_codes['pylint_error'].apply(lambda x: x['score'] if x else None)\n",
    "    python_codes['pylint_severity'] = python_codes['pylint_error'].apply(lambda x: x['severity'] if x else None)\n",
    "    python_codes['pylint_messages'] = python_codes['pylint_error'].apply(lambda x: x['messages'] if x else None)\n",
    "\n",
    "print(python_codes.pylint_severity.value_counts())\n",
    "print(python_codes.groupby('pylint_severity')['pylint_score'].mean())\n",
    "\n",
    "print(pd.crosstab(python_codes['is_valid_python_with_ruff'], python_codes['is_valid_python_with_pylint']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(python_codes['code'].loc[648])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method = 'pylint'\n",
    "# python_codes[python_codes[f'is_valid_python_with_{method}'] == False][['code', \n",
    "# 'compile_error', '' f'{method}_error']].head(10)\n",
    "# python_codes[python_codes['is_valid_python_with_pylint'] == False][['code', 'pyflakes_error', 'ruff_error', 'pylint_score', 'pylint_severity', 'pylint_messages']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_codes[(python_codes.is_valid_python_with_ruff == False) & (python_codes.is_valid_python_with_pylint == True)][['code', 'ruff_error' ,'pylint_messages']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_error_category(error: str, error_categories: list) -> str:\n",
    "    \n",
    "    for category in error_categories:\n",
    "        if error is not None:\n",
    "            if category in str(error):\n",
    "                return category\n",
    "    return None\n",
    "\n",
    "pyflakes_error_categories = ['undefined name', 'assigned to but never used', 'imported but unused']\n",
    "python_codes['pyflakes_error_category'] = python_codes['pyflakes_error'].apply(get_error_category, error_categories=pyflakes_error_categories)\n",
    "python_codes.loc[python_codes.is_valid_python_with_compile == False, 'pyflakes_error_category'] = 'Invalid Syntax'\n",
    "python_codes.loc[(python_codes.is_valid_python_with_pyflakes == False) & (python_codes.pyflakes_error_category.isnull()), 'pyflakes_error_category'] = 'Other'\n",
    "\n",
    "python_codes['pyflakes_error_category'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruff_error_categories = [\"{None}\", \"{'F821'}\", \"{'F822'}\", \"{'F823'}\"]\n",
    "python_codes['ruff_error_category'] = python_codes['ruff_error'].apply(get_error_category, error_categories=ruff_error_categories)\n",
    "python_codes['ruff_error_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_codes[(python_codes.is_valid_python_with_ruff == True) & (python_codes.pyflakes_error_category == 'undefined name')][['code', 'ruff_error', 'pyflakes_error']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python_codes.to_csv('/mnt/foundation-shared/nina_xu_gretel_ai/datasets/python_codes_with_checks_0927.csv', index=False)\n",
    "python_codes = pd.read_csv('/mnt/foundation-shared/nina_xu_gretel_ai/datasets/python_codes_with_checks_0927.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python_codes[python_codes.is_valid_python_with_pyflakes == False][['code', 'pyflakes_error', 'is_valid_python_with_compile']].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile errors\n",
    "ind = 15\n",
    "ind = 115\n",
    "# pyflakes errors\n",
    "ind = 2 # imported but unused\n",
    "ind = 69 # assigned to but never used\n",
    "ind = 36 # undefined name\n",
    "# mypy errors\n",
    "ind = 576 # missing positional argument\n",
    "ind = 743 # unsupported operand types\n",
    "ind = 545 # has no attribute X\n",
    "# incomplete code\n",
    "ind = 261\n",
    "\n",
    "ind = 509\n",
    "print(python_codes.prompt[ind])\n",
    "print('----------\\n')\n",
    "print(python_codes.code[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_codes.error_category[(python_codes.is_valid_python_with_mypy == False)].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_codes[(python_codes.is_valid_python_with_mypy == False)][['mypy_error', 'pyflakes_error_category']].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python_codes[python_codes.pyflakes_error_category == 'undefined name'][['pyflakes_error', 'mypy_error']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_codes['incomplete_code']= python_codes.code.apply(lambda x: '# ...' in x)\n",
    "print(python_codes.incomplete_code.value_counts())\n",
    "python_codes[python_codes.incomplete_code == True][['code', 'pyflakes_error']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "navhelpers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
